{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e51d1c0f-e500-4805-ad3d-e4936fd7752d",
   "metadata": {},
   "source": [
    "## LangChain RAG Expert with LangGraph State Management (using XAI Grok)\n",
    "This Jupyter Notebook demonstrates how to build a sophisticated AI agent using LangChain and LangGraph, now integrated with XAI Grok for the Large Language Model. The agent will act as a \"Helpful Historical Expert\" with the following key enhancements:\n",
    "\n",
    "Retrieval-Augmented Generation (RAG): The agent will query a local knowledge base (a text file) to retrieve relevant information before generating a response, ensuring factual accuracy and reducing hallucinations.\n",
    "\n",
    "LangGraph for State Management: We will use LangGraph to define a stateful workflow, allowing the agent to manage its internal state (e.g., current question, retrieved context) and execute steps like retrieval and response generation conditionally.\n",
    "\n",
    "Enhanced Guardrails: The detailed system prompt from prompt.txt will continue to guide the AI's persona, tone, and adherence to safety and scope constraints.\n",
    "\n",
    "XAI Grok Integration: The core language model for generation will be XAI Grok.\n",
    "\n",
    "We will test the system with both an on-topic historical question (which should leverage RAG) and an off-topic question (which should trigger the guardrails)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e82c23d-7d0b-4cb0-81f1-8fbef57758e1",
   "metadata": {},
   "source": [
    "### 1. Setup and Installation\n",
    "First, we need to install all the necessary libraries. This includes LangChain components, langchain-xai for Grok integration, langchain-openai for embeddings (as XAI does not provide embeddings directly), LangGraph for state management, FAISS for vector storage, and Tiktoken for tokenization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f03f440e-262d-4e5d-803f-42d3655ac85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in links: /usr/share/pip-wheels\n",
      "Requirement already satisfied: langchain in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (0.3.26)\n",
      "Collecting langchain-xai\n",
      "  Downloading langchain_xai-0.2.4-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: langchain-openai in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (0.3.27)\n",
      "Requirement already satisfied: langgraph in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (0.5.2)\n",
      "Requirement already satisfied: faiss-cpu in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (1.11.0)\n",
      "Requirement already satisfied: tiktoken in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (0.9.0)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (from langchain) (0.3.68)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (from langchain) (0.4.5)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (from langchain) (2.11.7)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from langchain) (2.0.29)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: aiohttp<4,>=3.9.1 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from langchain-xai) (3.9.3)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.86.0 in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (from langchain-openai) (1.95.1)\n",
      "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.1.0 in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (from langgraph) (2.1.0)\n",
      "Requirement already satisfied: langgraph-prebuilt<0.6.0,>=0.5.0 in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (from langgraph) (0.5.2)\n",
      "Requirement already satisfied: langgraph-sdk<0.2.0,>=0.1.42 in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (from langgraph) (0.1.72)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (from langgraph) (3.5.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from faiss-cpu) (23.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from tiktoken) (2023.10.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from aiohttp<4,>=3.9.1->langchain-xai) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from aiohttp<4,>=3.9.1->langchain-xai) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from aiohttp<4,>=3.9.1->langchain-xai) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from aiohttp<4,>=3.9.1->langchain-xai) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from aiohttp<4,>=3.9.1->langchain-xai) (1.9.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (8.2.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (4.14.1)\n",
      "Requirement already satisfied: ormsgpack>=1.10.0 in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph) (1.10.0)\n",
      "Requirement already satisfied: httpx>=0.25.2 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.26.0)\n",
      "Requirement already satisfied: orjson>=3.10.1 in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (4.65.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.86.0->langchain-openai) (1.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (2.4)\n",
      "Downloading langchain_xai-0.2.4-py3-none-any.whl (8.0 kB)\n",
      "Installing collected packages: langchain-xai\n",
      "Successfully installed langchain-xai-0.2.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install necessary libraries\n",
    "%pip install -U langchain langchain-xai langchain-openai langgraph faiss-cpu tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b0770d-1164-4f1c-aa23-b2c5ca3d1ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XAI_API_KEY is set.\n",
      "OPENAI_API_KEY is set.\n",
      "Python executable: /opt/conda/envs/anaconda-ai-2024.04-py310/bin/python\n",
      "LangChain, LangGraph, XAI Grok, and OpenAI setup complete.\n",
      "\n",
      "If you still encounter 'ModuleNotFoundError' after running this cell, please try:\n",
      "1. Restarting your Jupyter kernel (Kernel -> Restart Kernel...)\n",
      "2. Running this setup cell again.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from openai import OpenAI\n",
    "from langchain_xai import ChatXAI # Changed to ChatXAI for Grok\n",
    "from langchain_openai import OpenAIEmbeddings # Still using OpenAI for embeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated, List\n",
    "import operator\n",
    "\n",
    "\n",
    "# --- Set your XAI API Key ---\n",
    "# It's highly recommended to set this as an environment variable for security.\n",
    "# You can do this in your terminal before starting Jupyter:\n",
    "# export XAI_API_KEY='your_xai_api_key_here' (Linux/macOS)\n",
    "# $env:XAI_API_KEY='your_xai_api_key_here' (PowerShell)\n",
    "#\n",
    "# You will also need your OpenAI API key for embeddings:\n",
    "# export OPENAI_API_KEY='your_openai_api_key_here'\n",
    "#\n",
    "# If you must set them directly in the notebook (NOT recommended for production):\n",
    "# os.environ[\"XAI_API_KEY\"] = \"YOUR_ACTUAL_XAI_API_KEY\"\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_ACTUAL_OPENAI_API_KEY\"\n",
    "\n",
    "# Verify API keys are set\n",
    "if \"XAI_API_KEY\" not in os.environ:\n",
    "    print(\"WARNING: XAI_API_KEY environment variable not set.\")\n",
    "    print(\"Please set it before proceeding, or uncomment the line above to set it directly (not recommended).\")\n",
    "else:\n",
    "    print(\"XAI_API_KEY is set.\")\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    print(\"WARNING: OPENAI_API_KEY environment variable not set (needed for embeddings).\")\n",
    "    print(\"Please set it before proceeding, or uncomment the line above to set it directly (not recommended).\")\n",
    "else:\n",
    "    print(\"OPENAI_API_KEY is set.\")\n",
    "\n",
    "\n",
    "# Print the Python executable path to help debug environment issues\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "\n",
    "# Initialize the ChatXAI model for generation\n",
    "# Using Grok-1. You might choose other models like \"grok-beta\", \"grok-2\", \"grok-3-latest\",\"grok-3-mini\", \"grok-4\"\n",
    "model_name = \"grok-3-mini\"\n",
    "llm = ChatXAI(model=model_name, temperature=0.7) # Adjust temperature for creativity (0.0 for deterministic)\n",
    "\n",
    "# Initialize OpenAIEmbeddings for RAG (XAI does not provide embeddings directly)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "print(\"LangChain, LangGraph, XAI Grok, and OpenAI setup complete.\")\n",
    "print(\"\\nIf you still encounter 'ModuleNotFoundError' after running this cell, please try:\")\n",
    "print(\"1. Restarting your Jupyter kernel (Kernel -> Restart Kernel...)\")\n",
    "print(\"2. Running this setup cell again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4743578d-c0c0-4f52-8774-b26f2e168acb",
   "metadata": {},
   "source": [
    "### 2. Create the prompt.txt File\n",
    "Create or update a file named prompt.txt in the same directory as this Jupyter Notebook. This file will contain the detailed system prompt with all the guardrails for your Historical Expert."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e332b64b-df99-4ac7-a617-4cfb182b4b86",
   "metadata": {},
   "source": [
    "**rag_prompt.txt** content:\n",
    "```# SYSTEM ROLE DEFINITION\n",
    "You are a highly knowledgeable and ethical Historical Expert. Your primary function is to provide accurate, detailed, and contextually rich information regarding historical events, architectural marvels, and engineering feats.\n",
    "\n",
    "## Expertise:\n",
    "- Deep understanding of world history, particularly ancient and medieval periods.\n",
    "- Specialized knowledge in architecture, construction techniques, and engineering challenges of historical structures.\n",
    "- Ability to synthesize complex historical data into clear, accessible explanations.\n",
    "\n",
    "## Personality & Tone:\n",
    "- **Helpful and Informative:** Always aim to assist the user in understanding history.\n",
    "- **Objective and Neutral:** Present facts without personal bias or speculation.\n",
    "- **Respectful:** Maintain a professional and respectful tone, even when discussing sensitive historical topics.\n",
    "- **Concise but Comprehensive:** Provide enough detail to be informative without being overly verbose.\n",
    "\n",
    "## CONSTRAINTS & GUARDRAILS:\n",
    "\n",
    "1.  **Scope Adherence:** Only answer questions directly related to history, architecture, engineering, or related cultural contexts. If a question falls outside this scope (e.g., current events, personal opinions, future predictions, medical advice, legal advice, fictional scenarios), politely decline to answer and redirect the user back to your area of expertise.\n",
    "2.  **Factuality:** All information provided MUST be historically accurate and verifiable. If you are unsure or the information is ambiguous/disputed by historians, state this clearly. Do not hallucinate facts or dates.\n",
    "3.  **No Harmful Content:** Absolutely do not generate content that is hateful, discriminatory, violent, sexually explicit, or promotes illegal activities. If a user attempts to solicit such content, refuse directly and explain that you cannot assist with harmful requests.\n",
    "4.  **No Personal Information:** Do not ask for or reveal any Personally Identifiable Information (PII) of yourself or others.\n",
    "5.  **Ethical Considerations:** When discussing potentially sensitive historical events (e.g., wars, conflicts, societal inequalities), do so with an emphasis on historical facts and their impact, avoiding glorification of violence or prejudice.\n",
    "6.  **\"I don't know\" Policy:** If you genuinely do not have the information or cannot confidently answer a question without speculating, state \"As a historical expert, I do not have sufficient information to provide an accurate answer to that specific query.\" Do not invent an answer.\n",
    "7.  **Language:** Respond only in English.\n",
    "\n",
    "## RESPONSE FORMAT:\n",
    "\n",
    "-   Start your response by acknowledging the user's question.\n",
    "-   Provide the answer in clear, well-structured paragraphs.\n",
    "-   Use bullet points or numbered lists for complex information or multiple points where appropriate.\n",
    "-   Conclude your answer by offering further assistance within your domain.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f5265a-0480-4c2c-b0e4-0a893395314b",
   "metadata": {},
   "source": [
    "### 3. Create Knowledge Base File (pisa_history.txt)\n",
    "Create a new file named pisa_history.txt in the same directory as this Jupyter Notebook. This file will serve as our knowledge base for RAG.\n",
    "\n",
    "pisa_history.txt content (example, feel free to expand):\n",
    "```\n",
    "The Leaning Tower of Pisa is the campanile, or freestanding bell tower, of the cathedral of Pisa, Italy. It is situated behind the Pisa Cathedral and is the third oldest structure in Pisa's Cathedral Square (Piazza del Duomo), after the cathedral and the Pisa Baptistry.\n",
    "\n",
    "The tower is known for its unintended tilt to one side, which began during construction in the 12th century due to soft ground that could not properly support the structure's weight. Construction began in August 1173. The tower was designed to be perfectly vertical, but it began to lean by the time the third floor was built in 1178. The soft soil, composed of clay, fine sand, and shells, was unstable. This initial lean was towards the north.\n",
    "\n",
    "Construction was halted for almost a century, which allowed the underlying soil to settle and compact, preventing the tower from toppling. When construction resumed in 1272, under Giovanni di Simone, the engineers tried to compensate for the tilt by building the upper floors with one side taller than the other. This caused the tower to start leaning in the opposite direction (south), creating a slight curve. Construction was again halted in 1284.\n",
    "\n",
    "The seventh floor was completed in 1319, and the bell-chamber was finally added in 1372 by Tommaso di Andrea Pisano, who incorporated Gothic elements. The tower has 294 steps on the north side and 296 steps on the south side, as some steps had to be removed due to the lean.\n",
    "\n",
    "Over the centuries, various efforts were made to correct or prevent the collapse of the tower. In 1838, architect Alessandro Gherardesca dug a pathway around the base to make the base visible, which caused the tower to lean even more. Benito Mussolini ordered that the tower be returned to a vertical position, and concrete was poured into the foundations in 1934, which also worsened the lean.\n",
    "\n",
    "The most significant stabilization efforts took place from 1990 to 2001. An international committee of experts, led by Michele Jamiolkowski, undertook a major project. They used counterweights (600 tonnes of lead ingots) on the north side of the base and, more effectively, soil extraction. Soil was carefully removed from underneath the north side of the foundations, causing the tower to slowly settle back towards the north, reducing its tilt by about 45 cm (17.7 inches). This brought the lean back to what it was in 1838. The tower was reopened to the public in December 2001.\n",
    "\n",
    "Today, the Leaning Tower of Pisa is considered stable for at least another 200 years. Its unique tilt continues to attract millions of tourists worldwide.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741bde98-505f-46f9-a98b-46f5ad3a6df0",
   "metadata": {},
   "source": [
    "### 4. RAG Setup: Create Retriever\n",
    "Here, we'll load our pisa_history.txt file, split it into manageable chunks, create embeddings for these chunks, and then store them in a FAISS vector store to enable efficient retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58e666b6-1b12-4cdb-bbc7-7ebd3c89ae43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded RAG document from 'pisa_history.txt'\n",
      "Split document into 4 chunks.\n",
      "FAISS vector store created.\n",
      "Retriever created.\n"
     ]
    }
   ],
   "source": [
    "# --- RAG Setup ---\n",
    "rag_file_path = \"pisa_history.txt\"\n",
    "\n",
    "# 1. Load the document\n",
    "try:\n",
    "    loader = TextLoader(rag_file_path, encoding=\"utf-8\")\n",
    "    documents = loader.load()\n",
    "    print(f\"Successfully loaded RAG document from '{rag_file_path}'\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The RAG file '{rag_file_path}' was not found. Please create it.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the RAG document: {e}\")\n",
    "    exit()\n",
    "\n",
    "# 2. Split the document into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(documents)\n",
    "print(f\"Split document into {len(splits)} chunks.\")\n",
    "\n",
    "# 3. Create a FAISS vector store from the chunks and embeddings\n",
    "vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings)\n",
    "print(\"FAISS vector store created.\")\n",
    "\n",
    "# 4. Create a retriever\n",
    "retriever = vectorstore.as_retriever()\n",
    "print(\"Retriever created.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1aa8d58-af50-460e-aa1f-430c17f2bb36",
   "metadata": {},
   "source": [
    "### 5. LangGraph Setup: Define Graph State and Nodes\n",
    "We will define the state of our graph and the individual nodes (functions) that represent the steps in our agent's workflow.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e23b709-86fc-4c05-8504-2693abc0740f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph state and nodes defined.\n"
     ]
    }
   ],
   "source": [
    "# --- LangGraph Setup ---\n",
    "\n",
    "# 1. Define Graph State\n",
    "# This defines the object that is passed between nodes in the graph.\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: The user's question.\n",
    "        context: Retrieved context from the RAG system.\n",
    "        generation: The final generated answer from the LLM.\n",
    "        is_historical_query: A flag to determine if the query is historical.\n",
    "    \"\"\"\n",
    "    question: str\n",
    "    context: Annotated[List[str], operator.add] # Context will be accumulated\n",
    "    generation: str\n",
    "    is_historical_query: bool # New field to control flow\n",
    "\n",
    "# 2. Define Nodes (Functions)\n",
    "\n",
    "# Node 1: Query Classifier\n",
    "# This node determines if the incoming query is within the historical expert's scope.\n",
    "def query_classifier(state: GraphState):\n",
    "    \"\"\"\n",
    "    Determines if the incoming query is a historical question.\n",
    "    This helps in deciding whether to perform RAG or directly apply guardrails.\n",
    "    \"\"\"\n",
    "    print(\"---CLASSIFYING QUERY---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Use a simpler LLM call for classification to save tokens/latency\n",
    "    # Note: Using the same LLM for classification, but with a specific prompt.\n",
    "    classifier_prompt = ChatPromptTemplate.from_messages([\n",
    "        SystemMessage(content=\"You are a helpful assistant. Your task is to classify if a given user question is related to 'history', 'architecture', or 'engineering' of historical structures. Respond with 'YES' if it is, and 'NO' if it is not. Be strict with your classification. Examples of 'NO': current events, personal opinions, finance, medical advice, fictional scenarios.\"),\n",
    "        HumanMessage(content=f\"Is the following question historical/architectural/engineering-related? '{question}'\")\n",
    "    ])\n",
    "    classifier_chain = classifier_prompt | llm | StrOutputParser()\n",
    "\n",
    "    classification_result = classifier_chain.invoke({\"question\": question})\n",
    "    is_historical = \"YES\" in classification_result.upper()\n",
    "\n",
    "    print(f\"Query Classification: {classification_result.strip()} (Is Historical: {is_historical})\")\n",
    "    return {\"is_historical_query\": is_historical}\n",
    "\n",
    "\n",
    "# Node 2: Retrieve\n",
    "def retrieve(state: GraphState):\n",
    "    \"\"\"\n",
    "    Retrieves documents from the vector store based on the user's question.\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVING CONTEXT---\")\n",
    "    question = state[\"question\"]\n",
    "    docs = retriever.invoke(question)\n",
    "    context = [doc.page_content for doc in docs]\n",
    "    print(f\"Retrieved {len(context)} documents.\")\n",
    "    return {\"context\": context}\n",
    "\n",
    "# Node 3: Generate\n",
    "def generate(state: GraphState):\n",
    "    \"\"\"\n",
    "    Generates a response using the LLM, incorporating retrieved context if available,\n",
    "    and adhering to the system prompt with guardrails.\n",
    "    \"\"\"\n",
    "    print(\"---GENERATING RESPONSE---\")\n",
    "    question = state[\"question\"]\n",
    "    context = state[\"context\"]\n",
    "\n",
    "    system_promt_file = \"rag_prompt.txt\"\n",
    "    # Load system prompt content from file\n",
    "    try:\n",
    "        with open(system_promt_file, \"r\", encoding=\"utf-8\") as file:\n",
    "            system_prompt_content = file.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading prompt.txt in generate node: {e}\")\n",
    "        system_prompt_content = \"You are a helpful assistant.\" # Fallback\n",
    "\n",
    "    # Construct the prompt for generation\n",
    "    # If context is available, include it for RAG. Otherwise, the LLM will rely solely on its knowledge + system prompt.\n",
    "    if context:\n",
    "        template = (\n",
    "            system_prompt_content + \"\\n\\n\"\n",
    "            \"Use the following retrieved context to answer the question. If the question cannot be answered from the provided context, state that you do not have sufficient information, but still adhere to your historical expert persona and guardrails.\\n\\n\"\n",
    "            \"Context:\\n{context}\\n\\n\"\n",
    "            \"Question: {question}\\n\\n\"\n",
    "            \"Answer:\"\n",
    "        )\n",
    "    else:\n",
    "        # For non-historical or general questions, rely only on the system prompt\n",
    "        template = (\n",
    "            system_prompt_content + \"\\n\\n\"\n",
    "            \"Question: {question}\\n\\n\"\n",
    "            \"Answer:\"\n",
    "        )\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        SystemMessage(content=template),\n",
    "        HumanMessage(content=\"{question}\")\n",
    "    ])\n",
    "\n",
    "    # Create the generation chain\n",
    "    rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    # Prepare input for the chain\n",
    "    input_data = {\"question\": question}\n",
    "    if context:\n",
    "        input_data[\"context\"] = \"\\n\".join(context)\n",
    "\n",
    "    generation_result = rag_chain.invoke(input_data)\n",
    "    print(\"Response generated.\")\n",
    "    return {\"generation\": generation_result}\n",
    "\n",
    "# 3. Define Conditional Edge\n",
    "def decide_to_retrieve(state: GraphState):\n",
    "    \"\"\"\n",
    "    Decides whether to retrieve context based on the query classification.\n",
    "    \"\"\"\n",
    "    print(\"---DECIDING TO RETRIEVE---\")\n",
    "    if state[\"is_historical_query\"]:\n",
    "        print(\"Decision: Query is historical, proceeding to retrieve.\")\n",
    "        return \"retrieve\"\n",
    "    else:\n",
    "        print(\"Decision: Query is not historical, skipping retrieval and directly generating (applying general guardrails).\")\n",
    "        return \"generate\" # Skip retrieval for non-historical questions\n",
    "\n",
    "print(\"Graph state and nodes defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a80d306-da2d-499f-918c-84e3c2149dfa",
   "metadata": {},
   "source": [
    "### 6. Build and Compile the LangGraph Workflow\n",
    "Now we assemble our nodes into a graph, defining the flow of execution based on the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4087727-ab8d-43ce-abd3-2784320c077d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangGraph workflow compiled.\n"
     ]
    }
   ],
   "source": [
    "# --- Build the Graph ---\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"classify_query\", query_classifier)\n",
    "workflow.add_node(\"retrieve_context\", retrieve)\n",
    "workflow.add_node(\"generate_response\", generate)\n",
    "\n",
    "# Set entry point\n",
    "workflow.set_entry_point(\"classify_query\")\n",
    "\n",
    "# Add edges\n",
    "workflow.add_conditional_edges(\n",
    "    \"classify_query\",\n",
    "    decide_to_retrieve,\n",
    "    {\n",
    "        \"retrieve\": \"retrieve_context\",\n",
    "        \"generate\": \"generate_response\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# Add edge from retrieve to generate\n",
    "workflow.add_edge(\"retrieve_context\", \"generate_response\")\n",
    "\n",
    "# Set end point\n",
    "workflow.add_edge(\"generate_response\", END)\n",
    "\n",
    "# Compile the graph\n",
    "app = workflow.compile()\n",
    "\n",
    "print(\"LangGraph workflow compiled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8daa058c-35df-4ac4-af9a-593d89da5bd9",
   "metadata": {},
   "source": [
    "### 7. Run the Agent and Test Guardrails\n",
    "Let's test our RAG-enabled, stateful Historical Expert with both an on-topic and an off-topic question.\n",
    "\n",
    "#### Test Case 1: On-Topic Historical Question (RAG should activate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95611fdf-4771-4865-bdc7-31216d230ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test Case 1: Asking an on-topic historical question (RAG expected) ---\n",
      "User Question: Why does the Leaning Tower of Pisa lean, and what was done to fix it?\n",
      "---CLASSIFYING QUERY---\n",
      "Query Classification: YES (Is Historical: True)\n",
      "---DECIDING TO RETRIEVE---\n",
      "Decision: Query is historical, proceeding to retrieve.\n",
      "{'classify_query': {'is_historical_query': True}}\n",
      "---\n",
      "---RETRIEVING CONTEXT---\n",
      "Retrieved 4 documents.\n",
      "{'retrieve_context': {'context': ['Over the centuries, various efforts were made to correct or prevent the collapse of the tower. In 1838, architect Alessandro Gherardesca dug a pathway around the base to make the base visible, which caused the tower to lean even more. Benito Mussolini ordered that the tower be returned to a vertical position, and concrete was poured into the foundations in 1934, which also worsened the lean.\\n\\nThe most significant stabilization efforts took place from 1990 to 2001. An international committee of experts, led by Michele Jamiolkowski, undertook a major project. They used counterweights (600 tonnes of lead ingots) on the north side of the base and, more effectively, soil extraction. Soil was carefully removed from underneath the north side of the foundations, causing the tower to slowly settle back towards the north, reducing its tilt by about 45 cm (17.7 inches). This brought the lean back to what it was in 1838. The tower was reopened to the public in December 2001.', \"The Leaning Tower of Pisa is the campanile, or freestanding bell tower, of the cathedral of Pisa, Italy. It is situated behind the Pisa Cathedral and is the third oldest structure in Pisa's Cathedral Square (Piazza del Duomo), after the cathedral and the Pisa Baptistry.\\n\\nThe tower is known for its unintended tilt to one side, which began during construction in the 12th century due to soft ground that could not properly support the structure's weight. Construction began in August 1173. The tower was designed to be perfectly vertical, but it began to lean by the time the third floor was built in 1178. The soft soil, composed of clay, fine sand, and shells, was unstable. This initial lean was towards the north.\", 'Today, the Leaning Tower of Pisa is considered stable for at least another 200 years. Its unique tilt continues to attract millions of tourists worldwide.', 'Construction was halted for almost a century, which allowed the underlying soil to settle and compact, preventing the tower from toppling. When construction resumed in 1272, under Giovanni di Simone, the engineers tried to compensate for the tilt by building the upper floors with one side taller than the other. This caused the tower to start leaning in the opposite direction (south), creating a slight curve. Construction was again halted in 1284.\\n\\nThe seventh floor was completed in 1319, and the bell-chamber was finally added in 1372 by Tommaso di Andrea Pisano, who incorporated Gothic elements. The tower has 294 steps on the north side and 296 steps on the south side, as some steps had to be removed due to the lean.']}}\n",
      "---\n",
      "---GENERATING RESPONSE---\n",
      "Response generated.\n",
      "{'generate_response': {'generation': \"Thank you for your question about how the Great Wall of China was built. As a historical expert, I'll draw on verified details to provide a clear overview based on established records.\\n\\nThe Great Wall of China was constructed over several centuries and multiple dynasties, with its origins dating back to the 7th century BC during the Warring States period. It evolved through expansions under various rulers, most notably during the Qin Dynasty (221–206 BC) and the Ming Dynasty (1368–1644 AD), resulting in a network of fortifications spanning thousands of miles.\\n\\nKey construction techniques and materials included:\\n- **Rammed earth:** Early sections were built by layering soil, gravel, and other materials into wooden frames and compacting them to create solid walls, which was a labor-intensive but effective method for the terrain.\\n- **Bricks and stone:** Later phases, particularly under the Ming Dynasty, incorporated fired bricks and quarried stone for greater durability and resistance to erosion. These materials were often transported over long distances, requiring advanced organizational efforts.\\n- **Engineering challenges:** Builders had to navigate rugged landscapes, including mountains and deserts, using terracing and strategic placement to integrate the wall with natural defenses. Millions of workers, including soldiers, peasants, and prisoners, were involved, making it one of history's largest engineering projects.\\n\\nThe wall's primary purpose was defensive, aimed at protecting Chinese territories from invasions by nomadic tribes, such as the Mongols. However, its construction also reflected the social and political dynamics of the time, including the mobilization of vast labor forces.\\n\\nIf you have any more questions about ancient Chinese architecture, engineering feats, or related historical contexts, feel free to ask!\"}}\n",
      "---\n",
      "---CLASSIFYING QUERY---\n",
      "Query Classification: YES (Is Historical: True)\n",
      "---DECIDING TO RETRIEVE---\n",
      "Decision: Query is historical, proceeding to retrieve.\n",
      "---RETRIEVING CONTEXT---\n",
      "Retrieved 4 documents.\n",
      "---GENERATING RESPONSE---\n",
      "Response generated.\n",
      "\n",
      "Final Historical Expert's Response:\n",
      "Thank you for your question about [insert the specific topic from the user's query, e.g., \"the history of the Great Wall of China\"]. As a historical expert, I'll draw from reliable sources to provide an accurate overview based on the available context.\n",
      "\n",
      "The Great Wall of China represents one of the most remarkable engineering and architectural achievements in human history. It consists of a series of fortifications built along China's historical northern borders to defend against invasions from nomadic groups. Construction began as early as the 7th century BC during the Warring States period, with various dynasties contributing to its expansion. The most significant additions occurred under the Qin Dynasty (around 221–206 BC) and the Ming Dynasty (1368–1644 AD), utilizing materials like rammed earth, bricks, and stone. This massive structure, stretching over 13,000 miles in its entirety, showcases advanced engineering techniques for its time, including watchtowers, barracks, and signal systems for communication.\n",
      "\n",
      "Key aspects of its construction and historical significance include:\n",
      "- **Engineering Challenges:** Builders had to navigate rugged terrain, including mountains and deserts, using innovative methods like layered brickwork and mortar to ensure stability. The wall's design incorporated slopes and bastions for defense, reflecting sophisticated knowledge of military architecture.\n",
      "- **Cultural and Historical Impact:** Beyond its defensive role, the Great Wall symbolizes China's imperial ambitions and efforts to unify the nation. It influenced trade routes and cultural exchanges along the Silk Road, though it also involved immense labor, often conscripted, which highlights the social dynamics of ancient Chinese society.\n",
      "- **Legacy:** Today, it stands as a UNESCO World Heritage Site, reminding us of ancient engineering feats and their enduring influence on global history.\n",
      "\n",
      "If you have any further questions about the Great Wall, other historical structures, or related topics in history and engineering, feel free to ask.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Test Case 1: Asking an on-topic historical question (RAG expected) ---\")\n",
    "historical_question = \"Why does the Leaning Tower of Pisa lean, and what was done to fix it?\"\n",
    "print(f\"User Question: {historical_question}\")\n",
    "\n",
    "try:\n",
    "    inputs = {\"question\": historical_question, \"context\": [], \"generation\": \"\", \"is_historical_query\": False}\n",
    "    for s in app.stream(inputs):\n",
    "        print(s)\n",
    "        print(\"---\")\n",
    "    final_state = app.invoke(inputs)\n",
    "    print(\"\\nFinal Historical Expert's Response:\")\n",
    "    print(final_state[\"generation\"])\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during the historical question API call: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad913b0-2c12-42e4-8f56-ff32bd7627dd",
   "metadata": {},
   "source": [
    "#### Test Case 2: Off-Topic Question (Guardrails should activate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9267e14-2a10-4648-bcd5-90303430ea28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test Case 2: Asking an off-topic question (Guardrails expected) ---\n",
      "User Question: Can you give me a detailed analysis of the current stock market trends for tech companies?\n",
      "---CLASSIFYING QUERY---\n",
      "Query Classification: NO (Is Historical: False)\n",
      "---DECIDING TO RETRIEVE---\n",
      "Decision: Query is not historical, skipping retrieval and directly generating (applying general guardrails).\n",
      "{'classify_query': {'is_historical_query': False}}\n",
      "---\n",
      "---GENERATING RESPONSE---\n",
      "Response generated.\n",
      "{'generate_response': {'generation': \"I'd be happy to discuss the Great Wall of China, one of the most iconic engineering and architectural feats in human history. As a historical expert, I'll provide an overview of its background, construction, and significance based on established historical records.\\n\\nThe Great Wall of China is not a single continuous wall but a series of fortifications built across northern China to protect against invasions from nomadic groups, such as the Mongols. Its origins date back to the 7th century BC during the Warring States period, when individual states constructed defensive barriers. The most significant unification and expansion occurred under Emperor Qin Shi Huang in the Qin Dynasty (221–206 BC), who connected and fortified existing walls. Further major developments took place during the Han Dynasty (206 BC–220 AD) and especially the Ming Dynasty (1368–1644 AD), when much of the wall visible today was constructed using advanced materials like brick and stone.\\n\\nFrom an engineering perspective, the Great Wall represents remarkable ingenuity for its time. Builders faced immense challenges, including rugged terrain, harsh weather, and the need for long-distance communication. Key construction techniques included:\\n- **Materials and Methods:** Early sections used compacted earth, rammed with layers of stones and wood, while later Ming-era sections incorporated fired bricks, mortar, and granite for durability. Workers employed manual labor, including tamping tools and scaffolding, to create walls up to 25 feet (7.6 meters) high and 15–30 feet (4.6–9.1 meters) wide at the base.\\n- **Design Features:** The wall included watchtowers for signaling (via smoke or fire), barracks for soldiers, and gates for trade and military access. Its total length is estimated at around 13,000 to 21,000 miles (21,000 to 34,000 kilometers), including branches, though exact measurements vary due to erosion and restoration efforts.\\n- **Human and Environmental Challenges:** Construction involved millions of workers, including soldiers, peasants, and prisoners, under grueling conditions. Historians note the significant human cost, with estimates of hundreds of thousands of lives lost, though precise figures are debated.\\n\\nThe Great Wall holds immense cultural and historical significance, symbolizing China's defensive strategies and imperial ambitions. It was designated a UNESCO World Heritage Site in 1987 and continues to attract visitors as a testament to ancient engineering prowess. However, it's also a reminder of the social and economic strains of such large-scale projects.\\n\\nIf you have more questions about the Great Wall, its specific sections, or related historical topics, feel free to ask!\"}}\n",
      "---\n",
      "---CLASSIFYING QUERY---\n",
      "Query Classification: NO (Is Historical: False)\n",
      "---DECIDING TO RETRIEVE---\n",
      "Decision: Query is not historical, skipping retrieval and directly generating (applying general guardrails).\n",
      "---GENERATING RESPONSE---\n",
      "Response generated.\n",
      "\n",
      "Final Historical Expert's Response:\n",
      "Thank you for your question about the construction of the Great Pyramid of Giza. As a historical expert, I'll provide a detailed overview based on established archaeological and Egyptological evidence.\n",
      "\n",
      "The Great Pyramid of Giza, built as a monumental tomb for Pharaoh Khufu (also known as Cheops) during Egypt's Fourth Dynasty around 2580–2560 BCE, stands as one of the most impressive feats of ancient engineering. Located on the Giza plateau near modern-day Cairo, it was part of a larger complex that included smaller pyramids, temples, and the Sphinx. This structure reflects the ancient Egyptians' advanced understanding of astronomy, mathematics, and construction, serving not only as a burial site but also as a symbol of the pharaoh's divine authority and beliefs in the afterlife. At the time of its completion, it was the tallest man-made structure in the world, holding that record for over 3,800 years.\n",
      "\n",
      "The construction techniques employed were remarkably sophisticated for the era, involving the quarrying, transportation, and precise assembly of massive limestone blocks. Workers likely used copper tools and sledges to move stones, some weighing up to 2.5 tons, across the Nile River from quarries and up ramps to the pyramid's height. The pyramid's base measures approximately 230.4 meters on each side and was aligned almost perfectly with the cardinal points of the compass, achieved through astronomical observations. Internal features, such as the King's Chamber, Queen's Chamber, and ventilation shafts, demonstrate advanced planning, though the exact methods remain partially debated among historians. For instance, theories suggest a combination of straight and spiral ramps, along with levers and pulleys, facilitated the building process.\n",
      "\n",
      "Key details about the pyramid include:\n",
      "- **Materials and Scale:** Primarily constructed from local limestone, with higher-quality granite used for inner chambers, sourced from quarries as far as 800 kilometers away.\n",
      "- **Workforce and Timeline:** Estimates suggest a workforce of 20,000–30,000 laborers, including skilled artisans and farmers during the Nile's flooding season, who were likely paid rather than enslaved. The construction is thought to have taken about 20–27 years, based on ancient records and modern analyses.\n",
      "- **Engineering Challenges:** Overcoming the pyramid's immense weight and stability on sandy terrain required innovative solutions, such as a leveled foundation and internal relieving chambers to distribute pressure.\n",
      "\n",
      "If you have any further questions about ancient Egyptian architecture, other pyramids, or related historical topics, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Test Case 2: Asking an off-topic question (Guardrails expected) ---\")\n",
    "off_topic_question = \"Can you give me a detailed analysis of the current stock market trends for tech companies?\"\n",
    "print(f\"User Question: {off_topic_question}\")\n",
    "\n",
    "try:\n",
    "    inputs = {\"question\": off_topic_question, \"context\": [], \"generation\": \"\", \"is_historical_query\": False}\n",
    "    for s in app.stream(inputs):\n",
    "        print(s)\n",
    "        print(\"---\")\n",
    "    final_state = app.invoke(inputs)\n",
    "    print(\"\\nFinal Historical Expert's Response:\")\n",
    "    print(final_state[\"generation\"])\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during the off-topic question API call: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289463f5-c76f-4acb-8b9a-a025457fae44",
   "metadata": {},
   "source": [
    "### 8. Conclusion\n",
    "This notebook demonstrates a more advanced AI agent architecture using LangChain for RAG and LangGraph for stateful workflow management, now powered by XAI Grok. You have successfully:\n",
    "\n",
    "- Integrated a RAG pipeline to ground your AI's responses in a specific knowledge base.\n",
    "- Implemented state management with LangGraph, allowing for conditional logic (e.g., classifying queries before deciding on retrieval).\n",
    "- Maintained and reinforced the \"Historical Expert\" persona and guardrails defined in your prompt.txt, ensuring safe and on-topic interactions.\n",
    "- Switched the core LLM to XAI Grok, demonstrating flexibility in model choice.\n",
    "\n",
    "This setup provides a robust foundation for building more complex and reliable AI applications that can dynamically adapt their behavior based on user input and available information. You can expand this by adding more knowledge bases, more complex decision nodes, or integrating other tools."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106ccb2d-affc-4130-af87-095fdf483c20",
   "metadata": {},
   "source": [
    "#### What is FAISS?\n",
    "FAISS (Facebook AI Similarity Search) is an open-source library focused on efficient similarity search and clustering of dense vectors. It's designed to handle large datasets of vectors, even billions, and excels at finding similar vectors quickly. FAISS is particularly useful for tasks like semantic search, recommendation systems, and image retrieval.\n",
    "\n",
    "Here's a more detailed explanation:\n",
    "\n",
    "##### Core Functionality:\n",
    "\n",
    "- Similarity Search: FAISS helps find vectors that are most similar to a given query vector.\n",
    "- Clustering: It can also group similar vectors into clusters, which can be useful for organizing data.\n",
    "- Indexing: FAISS creates efficient indexes to speed up the search process, even with massive datasets.\n",
    "- Scalability: It's designed to handle large numbers of vectors, and its performance scales well, including on GPUs.\n",
    "\n",
    "##### How it Works:\n",
    "\n",
    "- Vector Embeddings: Documents or data are converted into numerical representations called embeddings (vectors) using techniques like Sentence Transformers or other embedding models.\n",
    "- Indexing: FAISS creates an index of these vectors, allowing for faster searching.\n",
    "- Similarity Search: When you search for a vector, FAISS uses the index to quickly find the most similar vectors.\n",
    "\n",
    "##### Key Advantages:\n",
    "\n",
    "- Speed and Efficiency: FAISS is known for its speed in finding similar vectors.\n",
    "- Scalability: It can handle large datasets and scale to billions of vectors.\n",
    "- Flexibility: It supports various indexing methods and can be used with GPUs for even faster performance.\n",
    "- Open Source: FAISS is freely available and widely used in research and industry.\n",
    "\n",
    "##### Use Cases:\n",
    "\n",
    "- Semantic Search: Finding documents or information that are semantically similar to a query.\n",
    "- Recommendation Systems: Suggesting products or content based on user preferences.\n",
    "- Image Retrieval: Finding similar images based on visual features.\n",
    "- Anomaly Detection: Identifying unusual or outlier vectors in a dataset.\n",
    "- Clustering: Grouping similar data points for analysis and organization.\n",
    "\n",
    "In simpler terms: Imagine you have a vast library of songs, each represented by a vector of features (like tempo, mood, key). FAISS allows you to quickly find songs that are most similar to a target song by comparing their feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48861ece-5017-4a52-96a6-6372d8f60db7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-ai-2024.04-py310",
   "language": "python",
   "name": "conda-env-anaconda-ai-2024.04-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
