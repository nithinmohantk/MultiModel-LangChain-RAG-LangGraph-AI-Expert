{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19c38c3a-ceed-4d49-878d-9accb41b8302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in links: /usr/share/pip-wheels\n",
      "Requirement already satisfied: langchain in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (0.3.26)\n",
      "Requirement already satisfied: langchain-openai in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (0.3.27)\n",
      "Collecting langgraph\n",
      "  Downloading langgraph-0.5.2-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.11.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: tiktoken in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (0.9.0)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (from langchain) (0.3.68)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (from langchain) (0.4.5)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (from langchain) (2.11.7)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from langchain) (2.0.29)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.86.0 in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (from langchain-openai) (1.95.1)\n",
      "Collecting langgraph-checkpoint<3.0.0,>=2.1.0 (from langgraph)\n",
      "  Downloading langgraph_checkpoint-2.1.0-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting langgraph-prebuilt<0.6.0,>=0.5.0 (from langgraph)\n",
      "  Downloading langgraph_prebuilt-0.5.2-py3-none-any.whl.metadata (4.5 kB)\n",
      "Collecting langgraph-sdk<0.2.0,>=0.1.42 (from langgraph)\n",
      "  Downloading langgraph_sdk-0.1.72-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting xxhash>=3.5.0 (from langgraph)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from faiss-cpu) (23.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from tiktoken) (2023.10.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (8.2.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (4.14.1)\n",
      "Collecting ormsgpack>=1.10.0 (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph)\n",
      "  Downloading ormsgpack-1.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[38;5;70m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m820.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: httpx>=0.25.2 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.26.0)\n",
      "Collecting orjson>=3.10.1 (from langgraph-sdk<0.2.0,>=0.1.42->langgraph)\n",
      "  Downloading orjson-3.10.18-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n",
      "\u001b[2K     \u001b[38;5;70m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.9/41.9 kB\u001b[0m \u001b[31m780.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (4.65.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.86.0->langchain-openai) (1.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (2.4)\n",
      "Downloading langgraph-0.5.2-py3-none-any.whl (143 kB)\n",
      "\u001b[2K   \u001b[38;5;70m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.7/143.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading faiss_cpu-1.11.0-cp310-cp310-manylinux_2_28_x86_64.whl (31.3 MB)\n",
      "\u001b[2K   \u001b[38;5;70m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading langgraph_checkpoint-2.1.0-py3-none-any.whl (43 kB)\n",
      "\u001b[2K   \u001b[38;5;70m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m742.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading langgraph_prebuilt-0.5.2-py3-none-any.whl (23 kB)\n",
      "Downloading langgraph_sdk-0.1.72-py3-none-any.whl (50 kB)\n",
      "\u001b[2K   \u001b[38;5;70m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m836.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[38;5;70m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading orjson-3.10.18-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (132 kB)\n",
      "\u001b[2K   \u001b[38;5;70m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.8/132.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading ormsgpack-1.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (216 kB)\n",
      "\u001b[2K   \u001b[38;5;70m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.5/216.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xxhash, ormsgpack, orjson, faiss-cpu, langgraph-sdk, langgraph-checkpoint, langgraph-prebuilt, langgraph\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-community 0.0.32 requires langchain-core<0.2.0,>=0.1.41, but you have langchain-core 0.3.68 which is incompatible.\n",
      "langchain-community 0.0.32 requires langsmith<0.2.0,>=0.1.0, but you have langsmith 0.4.5 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed faiss-cpu-1.11.0 langgraph-0.5.2 langgraph-checkpoint-2.1.0 langgraph-prebuilt-0.5.2 langgraph-sdk-0.1.72 orjson-3.10.18 ormsgpack-1.10.0 xxhash-3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install necessary libraries\n",
    "%pip install -U langchain langchain-openai langgraph faiss-cpu tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ceccc39-325d-48d8-bb82-18045188a66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY is set.\n",
      "Python executable: /opt/conda/envs/anaconda-ai-2024.04-py310/bin/python\n",
      "LangChain, LangGraph, and OpenAI setup complete.\n",
      "\n",
      "If you still encounter 'ModuleNotFoundError' after running this cell, please try:\n",
      "1. Restarting your Jupyter kernel (Kernel -> Restart Kernel...)\n",
      "2. Running this setup cell again.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated, List\n",
    "import operator\n",
    "\n",
    "\n",
    "# --- Set your OpenAI API Key ---\n",
    "# It's highly recommended to set this as an environment variable for security.\n",
    "# You can do this in your terminal before starting Jupyter:\n",
    "# export OPENAI_API_KEY='your_api_key_here' (Linux/macOS)\n",
    "# $env:OPENAI_API_KEY='your_api_key_here' (PowerShell)\n",
    "#\n",
    "# If you must set it directly in the notebook (NOT recommended for production):\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_ACTUAL_OPENAI_API_KEY\"\n",
    "\n",
    "# Verify API key is set\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    print(\"WARNING: OPENAI_API_KEY environment variable not set.\")\n",
    "    print(\"Please set it before proceeding, or uncomment the line above to set it directly (not recommended).\")\n",
    "else:\n",
    "    print(\"OPENAI_API_KEY is set.\")\n",
    "\n",
    "# Print the Python executable path to help debug environment issues\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "\n",
    "# Initialize the ChatOpenAI model for generation\n",
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0.7) # Adjust temperature for creativity (0.0 for deterministic)\n",
    "\n",
    "# Initialize OpenAIEmbeddings for RAG\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "print(\"LangChain, LangGraph, and OpenAI setup complete.\")\n",
    "print(\"\\nIf you still encounter 'ModuleNotFoundError' after running this cell, please try:\")\n",
    "print(\"1. Restarting your Jupyter kernel (Kernel -> Restart Kernel...)\")\n",
    "print(\"2. Running this setup cell again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72caf842-a945-4429-8494-d0db6c5791b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded RAG document from 'pisa_history.txt'\n",
      "Split document into 4 chunks.\n",
      "FAISS vector store created.\n",
      "Retriever created.\n"
     ]
    }
   ],
   "source": [
    "# --- RAG Setup ---\n",
    "rag_file_path = \"pisa_history.txt\"\n",
    "\n",
    "# 1. Load the document\n",
    "try:\n",
    "    loader = TextLoader(rag_file_path, encoding=\"utf-8\")\n",
    "    documents = loader.load()\n",
    "    print(f\"Successfully loaded RAG document from '{rag_file_path}'\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The RAG file '{rag_file_path}' was not found. Please create it.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the RAG document: {e}\")\n",
    "    exit()\n",
    "\n",
    "# 2. Split the document into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(documents)\n",
    "print(f\"Split document into {len(splits)} chunks.\")\n",
    "\n",
    "# 3. Create a FAISS vector store from the chunks and embeddings\n",
    "vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings)\n",
    "print(\"FAISS vector store created.\")\n",
    "\n",
    "# 4. Create a retriever\n",
    "retriever = vectorstore.as_retriever()\n",
    "print(\"Retriever created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7291d49-8767-4f71-b779-4f30dc45f5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph state and nodes defined.\n"
     ]
    }
   ],
   "source": [
    "# --- LangGraph Setup ---\n",
    "\n",
    "# 1. Define Graph State\n",
    "# This defines the object that is passed between nodes in the graph.\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: The user's question.\n",
    "        context: Retrieved context from the RAG system.\n",
    "        generation: The final generated answer from the LLM.\n",
    "        is_historical_query: A flag to determine if the query is historical.\n",
    "    \"\"\"\n",
    "    question: str\n",
    "    context: Annotated[List[str], operator.add] # Context will be accumulated\n",
    "    generation: str\n",
    "    is_historical_query: bool # New field to control flow\n",
    "\n",
    "# 2. Define Nodes (Functions)\n",
    "\n",
    "# Node 1: Query Classifier\n",
    "# This node determines if the user's question is within the historical expert's scope.\n",
    "def query_classifier(state: GraphState):\n",
    "    \"\"\"\n",
    "    Determines if the incoming query is a historical question.\n",
    "    This helps in deciding whether to perform RAG or directly apply guardrails.\n",
    "    \"\"\"\n",
    "    print(\"---CLASSIFYING QUERY---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Use a simpler LLM call for classification to save tokens/latency\n",
    "    classifier_prompt = ChatPromptTemplate.from_messages([\n",
    "        SystemMessage(content=\"You are a helpful assistant. Your task is to classify if a given user question is related to 'history', 'architecture', or 'engineering' of historical structures. Respond with 'YES' if it is, and 'NO' if it is not. Be strict with your classification. Examples of 'NO': current events, personal opinions, finance, medical advice, fictional scenarios.\"),\n",
    "        HumanMessage(content=f\"Is the following question historical/architectural/engineering-related? '{question}'\")\n",
    "    ])\n",
    "    classifier_chain = classifier_prompt | llm | StrOutputParser()\n",
    "\n",
    "    classification_result = classifier_chain.invoke({\"question\": question})\n",
    "    is_historical = \"YES\" in classification_result.upper()\n",
    "\n",
    "    print(f\"Query Classification: {classification_result.strip()} (Is Historical: {is_historical})\")\n",
    "    return {\"is_historical_query\": is_historical}\n",
    "\n",
    "\n",
    "# Node 2: Retrieve\n",
    "def retrieve(state: GraphState):\n",
    "    \"\"\"\n",
    "    Retrieves documents from the vector store based on the user's question.\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVING CONTEXT---\")\n",
    "    question = state[\"question\"]\n",
    "    docs = retriever.invoke(question)\n",
    "    context = [doc.page_content for doc in docs]\n",
    "    print(f\"Retrieved {len(context)} documents.\")\n",
    "    return {\"context\": context}\n",
    "\n",
    "# Node 3: Generate\n",
    "def generate(state: GraphState):\n",
    "    \"\"\"\n",
    "    Generates a response using the LLM, incorporating retrieved context if available,\n",
    "    and adhering to the system prompt with guardrails.\n",
    "    \"\"\"\n",
    "    print(\"---GENERATING RESPONSE---\")\n",
    "    question = state[\"question\"]\n",
    "    context = state[\"context\"]\n",
    "\n",
    "    # Load system prompt content from file\n",
    "    try:\n",
    "        with open(\"prompt.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "            system_prompt_content = file.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading prompt.txt in generate node: {e}\")\n",
    "        system_prompt_content = \"You are a helpful assistant.\" # Fallback\n",
    "\n",
    "    # Construct the prompt for generation\n",
    "    # If context is available, include it for RAG. Otherwise, the LLM will rely solely on its knowledge + system prompt.\n",
    "    if context:\n",
    "        template = (\n",
    "            system_prompt_content + \"\\n\\n\"\n",
    "            \"Use the following retrieved context to answer the question. If the question cannot be answered from the provided context, state that you do not have sufficient information, but still adhere to your historical expert persona and guardrails.\\n\\n\"\n",
    "            \"Context:\\n{context}\\n\\n\"\n",
    "            \"Question: {question}\\n\\n\"\n",
    "            \"Answer:\"\n",
    "        )\n",
    "    else:\n",
    "        # For non-historical or general questions, rely only on the system prompt\n",
    "        template = (\n",
    "            system_prompt_content + \"\\n\\n\"\n",
    "            \"Question: {question}\\n\\n\"\n",
    "            \"Answer:\"\n",
    "        )\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        SystemMessage(content=template),\n",
    "        HumanMessage(content=\"{question}\")\n",
    "    ])\n",
    "\n",
    "    # Create the generation chain\n",
    "    rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    # Prepare input for the chain\n",
    "    input_data = {\"question\": question}\n",
    "    if context:\n",
    "        input_data[\"context\"] = \"\\n\".join(context)\n",
    "\n",
    "    generation_result = rag_chain.invoke(input_data)\n",
    "    print(\"Response generated.\")\n",
    "    return {\"generation\": generation_result}\n",
    "\n",
    "# 3. Define Conditional Edge\n",
    "def decide_to_retrieve(state: GraphState):\n",
    "    \"\"\"\n",
    "    Decides whether to retrieve context based on the query classification.\n",
    "    \"\"\"\n",
    "    print(\"---DECIDING TO RETRIEVE---\")\n",
    "    if state[\"is_historical_query\"]:\n",
    "        print(\"Decision: Query is historical, proceeding to retrieve.\")\n",
    "        return \"retrieve\"\n",
    "    else:\n",
    "        print(\"Decision: Query is not historical, skipping retrieval and directly generating (applying general guardrails).\")\n",
    "        return \"generate\" # Skip retrieval for non-historical questions\n",
    "\n",
    "print(\"Graph state and nodes defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "419b964b-f5cb-4da5-8d63-9a7d59e57241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangGraph workflow compiled.\n"
     ]
    }
   ],
   "source": [
    "# --- Build the Graph ---\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"classify_query\", query_classifier)\n",
    "workflow.add_node(\"retrieve_context\", retrieve)\n",
    "workflow.add_node(\"generate_response\", generate)\n",
    "\n",
    "# Set entry point\n",
    "workflow.set_entry_point(\"classify_query\")\n",
    "\n",
    "# Add edges\n",
    "workflow.add_conditional_edges(\n",
    "    \"classify_query\",\n",
    "    decide_to_retrieve,\n",
    "    {\n",
    "        \"retrieve\": \"retrieve_context\",\n",
    "        \"generate\": \"generate_response\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# Add edge from retrieve to generate\n",
    "workflow.add_edge(\"retrieve_context\", \"generate_response\")\n",
    "\n",
    "# Set end point\n",
    "workflow.add_edge(\"generate_response\", END)\n",
    "\n",
    "# Compile the graph\n",
    "app = workflow.compile()\n",
    "\n",
    "print(\"LangGraph workflow compiled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3528ba41-a6e2-489c-b64d-d70e3e52a438",
   "metadata": {},
   "source": [
    "### 7. Run the Agent and Test Guardrails\n",
    "Let's test our RAG-enabled, stateful Historical Expert with both an on-topic and an off-topic question.\n",
    "\n",
    "Test Case 1: On-Topic Historical Question (RAG should activate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "192aefa0-c214-4c61-a5fe-29834282f5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test Case 1: Asking an on-topic historical question (RAG expected) ---\n",
      "User Question: Why does the Leaning Tower of Pisa lean, and what was done to fix it?\n",
      "---CLASSIFYING QUERY---\n",
      "Query Classification: YES (Is Historical: True)\n",
      "---DECIDING TO RETRIEVE---\n",
      "Decision: Query is historical, proceeding to retrieve.\n",
      "{'classify_query': {'is_historical_query': True}}\n",
      "---\n",
      "---RETRIEVING CONTEXT---\n",
      "Retrieved 4 documents.\n",
      "{'retrieve_context': {'context': ['Over the centuries, various efforts were made to correct or prevent the collapse of the tower. In 1838, architect Alessandro Gherardesca dug a pathway around the base to make the base visible, which caused the tower to lean even more. Benito Mussolini ordered that the tower be returned to a vertical position, and concrete was poured into the foundations in 1934, which also worsened the lean.\\n\\nThe most significant stabilization efforts took place from 1990 to 2001. An international committee of experts, led by Michele Jamiolkowski, undertook a major project. They used counterweights (600 tonnes of lead ingots) on the north side of the base and, more effectively, soil extraction. Soil was carefully removed from underneath the north side of the foundations, causing the tower to slowly settle back towards the north, reducing its tilt by about 45 cm (17.7 inches). This brought the lean back to what it was in 1838. The tower was reopened to the public in December 2001.', \"The Leaning Tower of Pisa is the campanile, or freestanding bell tower, of the cathedral of Pisa, Italy. It is situated behind the Pisa Cathedral and is the third oldest structure in Pisa's Cathedral Square (Piazza del Duomo), after the cathedral and the Pisa Baptistry.\\n\\nThe tower is known for its unintended tilt to one side, which began during construction in the 12th century due to soft ground that could not properly support the structure's weight. Construction began in August 1173. The tower was designed to be perfectly vertical, but it began to lean by the time the third floor was built in 1178. The soft soil, composed of clay, fine sand, and shells, was unstable. This initial lean was towards the north.\", 'Today, the Leaning Tower of Pisa is considered stable for at least another 200 years. Its unique tilt continues to attract millions of tourists worldwide.', 'Construction was halted for almost a century, which allowed the underlying soil to settle and compact, preventing the tower from toppling. When construction resumed in 1272, under Giovanni di Simone, the engineers tried to compensate for the tilt by building the upper floors with one side taller than the other. This caused the tower to start leaning in the opposite direction (south), creating a slight curve. Construction was again halted in 1284.\\n\\nThe seventh floor was completed in 1319, and the bell-chamber was finally added in 1372 by Tommaso di Andrea Pisano, who incorporated Gothic elements. The tower has 294 steps on the north side and 296 steps on the south side, as some steps had to be removed due to the lean.']}}\n",
      "---\n",
      "---GENERATING RESPONSE---\n",
      "Response generated.\n",
      "{'generate_response': {'generation': \"Apologies, but your question seems to be missing. Could you please provide more details? I'm here to help with queries related to history, architecture, engineering, or related cultural contexts.\"}}\n",
      "---\n",
      "---CLASSIFYING QUERY---\n",
      "Query Classification: YES (Is Historical: True)\n",
      "---DECIDING TO RETRIEVE---\n",
      "Decision: Query is historical, proceeding to retrieve.\n",
      "---RETRIEVING CONTEXT---\n",
      "Retrieved 4 documents.\n",
      "---GENERATING RESPONSE---\n",
      "Response generated.\n",
      "\n",
      "Final Historical Expert's Response:\n",
      "I'm sorry, but it seems there was a mistake with the input. Please provide a specific question related to history, architecture, or engineering for me to assist you with.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Test Case 1: Asking an on-topic historical question (RAG expected) ---\")\n",
    "historical_question = \"Why does the Leaning Tower of Pisa lean, and what was done to fix it?\"\n",
    "print(f\"User Question: {historical_question}\")\n",
    "\n",
    "try:\n",
    "    inputs = {\"question\": historical_question, \"context\": [], \"generation\": \"\", \"is_historical_query\": False}\n",
    "    for s in app.stream(inputs):\n",
    "        print(s)\n",
    "        print(\"---\")\n",
    "    final_state = app.invoke(inputs)\n",
    "    print(\"\\nFinal Historical Expert's Response:\")\n",
    "    print(final_state[\"generation\"])\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during the historical question API call: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3599f84f-ad47-487b-bb7f-1338e73be075",
   "metadata": {},
   "source": [
    "### Test Case 2: Off-Topic Question (Guardrails should activate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2104d079-b60e-4eac-9000-aef615359d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test Case 2: Asking an off-topic question (Guardrails expected) ---\n",
      "User Question: Can you give me a detailed analysis of the current stock market trends for tech companies?\n",
      "---CLASSIFYING QUERY---\n",
      "Query Classification: NO (Is Historical: False)\n",
      "---DECIDING TO RETRIEVE---\n",
      "Decision: Query is not historical, skipping retrieval and directly generating (applying general guardrails).\n",
      "{'classify_query': {'is_historical_query': False}}\n",
      "---\n",
      "---GENERATING RESPONSE---\n",
      "Response generated.\n",
      "{'generate_response': {'generation': \"I'm sorry, but I can't assist without knowing the question. Can you please provide more details?\"}}\n",
      "---\n",
      "---CLASSIFYING QUERY---\n",
      "Query Classification: NO (Is Historical: False)\n",
      "---DECIDING TO RETRIEVE---\n",
      "Decision: Query is not historical, skipping retrieval and directly generating (applying general guardrails).\n",
      "---GENERATING RESPONSE---\n",
      "Response generated.\n",
      "\n",
      "Final Historical Expert's Response:\n",
      "Apologies for the confusion, but it seems your question got lost. Could you please restate your query? I'm here to provide information related to historical events, architectural marvels, and engineering feats.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Test Case 2: Asking an off-topic question (Guardrails expected) ---\")\n",
    "off_topic_question = \"Can you give me a detailed analysis of the current stock market trends for tech companies?\"\n",
    "print(f\"User Question: {off_topic_question}\")\n",
    "\n",
    "try:\n",
    "    inputs = {\"question\": off_topic_question, \"context\": [], \"generation\": \"\", \"is_historical_query\": False}\n",
    "    for s in app.stream(inputs):\n",
    "        print(s)\n",
    "        print(\"---\")\n",
    "    final_state = app.invoke(inputs)\n",
    "    print(\"\\nFinal Historical Expert's Response:\")\n",
    "    print(final_state[\"generation\"])\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during the off-topic question API call: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1ee3e7-06fd-4376-a5b5-0eb17cb01fac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-ai-2024.04-py310",
   "language": "python",
   "name": "conda-env-anaconda-ai-2024.04-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
