{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89c716c1-6924-4820-8574-74b0e42fa963",
   "metadata": {},
   "source": [
    "## LangChain RAG Expert with LangGraph State Management (using Gemini)\n",
    "This Jupyter Notebook demonstrates how to build a sophisticated AI agent using LangChain and LangGraph, now integrated with Google's Gemini API for the Large Language Model. The agent will act as a \"Helpful Historical Expert\" with the following key enhancements:\n",
    "\n",
    "Retrieval-Augmented Generation (RAG): The agent will query a local knowledge base (a text file) to retrieve relevant information before generating a response, ensuring factual accuracy and reducing hallucinations.\n",
    "\n",
    "LangGraph for State Management: We will use LangGraph to define a stateful workflow, allowing the agent to manage its internal state (e.g., current question, retrieved context) and execute steps like retrieval and response generation conditionally.\n",
    "\n",
    "Enhanced Guardrails: The detailed system prompt from prompt.txt will continue to guide the AI's persona, tone, and adherence to safety and scope constraints.\n",
    "\n",
    "Google Gemini Integration: The core language model for generation will be Google Gemini.\n",
    "\n",
    "We will test the system with both an on-topic historical question (which should leverage RAG) and an off-topic question (which should trigger the guardrails).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1286f8-dd54-4984-a37f-ec12db84b082",
   "metadata": {},
   "source": [
    "### 1. Setup and Installation\n",
    "First, we need to install all the necessary libraries. This includes LangChain components, langchain-google-genai for Gemini integration, langchain-openai for embeddings (as Gemini's native embeddings might require re-indexing the vector store, keeping OpenAI for consistency here), LangGraph for state management, FAISS for vector storage, and Tiktoken for tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7d0e8ea-0a5b-4568-bb89-fa0e05d1c4cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in links: /usr/share/pip-wheels\n",
      "Requirement already satisfied: langchain in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (0.3.26)\n",
      "Requirement already satisfied: langchain-google-genai in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (2.1.7)\n",
      "Requirement already satisfied: langchain-openai in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (0.3.27)\n",
      "Requirement already satisfied: langgraph in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (0.5.2)\n",
      "Requirement already satisfied: faiss-cpu in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (1.11.0)\n",
      "Requirement already satisfied: tiktoken in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (0.9.0)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (from langchain) (0.3.68)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (from langchain) (0.4.5)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (from langchain) (2.11.7)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from langchain) (2.0.29)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (from langchain-google-genai) (1.2.0)\n",
      "Requirement already satisfied: google-ai-generativelanguage<0.7.0,>=0.6.18 in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (from langchain-google-genai) (0.6.18)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.86.0 in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (from langchain-openai) (1.95.1)\n",
      "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.1.0 in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (from langgraph) (2.1.0)\n",
      "Requirement already satisfied: langgraph-prebuilt<0.6.0,>=0.5.0 in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (from langgraph) (0.5.2)\n",
      "Requirement already satisfied: langgraph-sdk<0.2.0,>=0.1.42 in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (from langgraph) (0.1.72)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (from langgraph) (3.5.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from faiss-cpu) (23.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from tiktoken) (2023.10.3)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.25.1)\n",
      "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.40.3)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.26.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (4.25.8)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (8.2.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (4.14.1)\n",
      "Requirement already satisfied: ormsgpack>=1.10.0 in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph) (1.10.0)\n",
      "Requirement already satisfied: httpx>=0.25.2 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.26.0)\n",
      "Requirement already satisfied: orjson>=3.10.1 in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (4.65.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.86.0->langchain-openai) (1.2.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.63.0)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.73.1)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.62.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (4.7.2)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (2.4)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (0.4.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install necessary libraries\n",
    "%pip install -U langchain langchain-google-genai langchain-openai langgraph faiss-cpu tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cd0761-5437-4727-a214-7916d9778eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOOGLE_API_KEY is set.\n",
      "OPENAI_API_KEY is set.\n",
      "Python executable: /opt/conda/envs/anaconda-ai-2024.04-py310/bin/python\n",
      "LangChain, LangGraph, Google Gemini, and OpenAI setup complete.\n",
      "\n",
      "If you still encounter 'ModuleNotFoundError' after running this cell, please try:\n",
      "1. Restarting your Jupyter kernel (Kernel -> Restart Kernel...)\n",
      "2. Running this setup cell again.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from openai import OpenAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI # Changed to ChatGoogleGenerativeAI for Gemini\n",
    "from langchain_openai import OpenAIEmbeddings # Still using OpenAI for embeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated, List\n",
    "import operator\n",
    "\n",
    "\n",
    "# --- Set your Google Gemini API Key ---\n",
    "# It's highly recommended to set this as an environment variable for security.\n",
    "# You can do this in your terminal before starting Jupyter:\n",
    "# export GOOGLE_API_KEY='your_google_api_key_here' (Linux/macOS)\n",
    "# $env:GOOGLE_API_KEY='your_google_api_key_here' (PowerShell)\n",
    "#\n",
    "# You will also need your OpenAI API key for embeddings:\n",
    "# export OPENAI_API_KEY='your_openai_api_key_here'\n",
    "#\n",
    "# If you must set them directly in the notebook (NOT recommended for production):\n",
    "# os.environ[\"GOOGLE_API_KEY\"] = \"YOUR_ACTUAL_GOOGLE_API_KEY\"\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_ACTUAL_OPENAI_API_KEY\"\n",
    "\n",
    "\n",
    "# Verify API keys are set\n",
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    print(\"WARNING: GOOGLE_API_KEY environment variable not set.\")\n",
    "    print(\"Please set it before proceeding, or uncomment the line above to set it directly (not recommended).\")\n",
    "else:\n",
    "    print(\"GOOGLE_API_KEY is set.\")\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    print(\"WARNING: OPENAI_API_KEY environment variable not set (needed for embeddings).\")\n",
    "    print(\"Please set it before proceeding, or uncomment the line above to set it directly (not recommended).\")\n",
    "else:\n",
    "    print(\"OPENAI_API_KEY is set.\")\n",
    "\n",
    "\n",
    "# Print the Python executable path to help debug environment issues\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "\n",
    "# Initialize the ChatGoogleGenerativeAI model for generation\n",
    "# Using gemini-pro. You might also use \"gemini-1.5-flash\" or \"gemini-2.5-flash\" or \"gemini-1.5-pro\" or \"gemini-2.5-pro\" if available and preferred.\n",
    "\n",
    "model_name = \"gemini-2.5-flash\"\n",
    "llm = ChatGoogleGenerativeAI(model=model_name, temperature=0.7) # Adjust temperature for creativity (0.0 for deterministic)\n",
    "\n",
    "# Initialize OpenAIEmbeddings for RAG (keeping consistent with previous notebooks)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "print(\"LangChain, LangGraph, Google Gemini, and OpenAI setup complete.\")\n",
    "print(\"\\nIf you still encounter 'ModuleNotFoundError' after running this cell, please try:\")\n",
    "print(\"1. Restarting your Jupyter kernel (Kernel -> Restart Kernel...)\")\n",
    "print(\"2. Running this setup cell again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aabcec8-c838-4f1a-b258-31c520a8c29d",
   "metadata": {},
   "source": [
    "### 2. Create the prompt.txt File\n",
    "Create or update a file named prompt.txt in the same directory as this Jupyter Notebook. This file will contain the detailed system prompt with all the guardrails for your Historical Expert.\n",
    "\n",
    "prompt.txt content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28e45fe-3527-4e8f-a9bc-0072be8af0e0",
   "metadata": {},
   "source": [
    "### 3. Create Knowledge Base File (pisa_history.txt)\n",
    "Create a new file named pisa_history.txt in the same directory as this Jupyter Notebook. This file will serve as our knowledge base for RAG.\n",
    "\n",
    "pisa_history.txt content (example, feel free to expand):\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7e23c0-1fa4-4749-8379-0c0d7c72069a",
   "metadata": {},
   "source": [
    "### 4. RAG Setup: Create Retriever\n",
    "Here, we'll load our pisa_history.txt file, split it into manageable chunks, create embeddings for these chunks, and then store them in a FAISS vector store to enable efficient retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf1251a3-beb2-48fc-8332-4f97cd50f159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded RAG document from 'pisa_history.txt'\n",
      "Split document into 4 chunks.\n",
      "FAISS vector store created.\n",
      "Retriever created.\n"
     ]
    }
   ],
   "source": [
    "# --- RAG Setup ---\n",
    "rag_file_path = \"pisa_history.txt\"\n",
    "\n",
    "# 1. Load the document\n",
    "try:\n",
    "    loader = TextLoader(rag_file_path, encoding=\"utf-8\")\n",
    "    documents = loader.load()\n",
    "    print(f\"Successfully loaded RAG document from '{rag_file_path}'\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The RAG file '{rag_file_path}' was not found. Please create it.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the RAG document: {e}\")\n",
    "    exit()\n",
    "\n",
    "# 2. Split the document into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(documents)\n",
    "print(f\"Split document into {len(splits)} chunks.\")\n",
    "\n",
    "# 3. Create a FAISS vector store from the chunks and embeddings\n",
    "vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings)\n",
    "print(\"FAISS vector store created.\")\n",
    "\n",
    "# 4. Create a retriever\n",
    "retriever = vectorstore.as_retriever()\n",
    "print(\"Retriever created.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525a4ce6-4a15-41e8-8981-a3e59cc1d93b",
   "metadata": {},
   "source": [
    "---\n",
    "### 5. LangGraph Setup: Define Graph State and Nodes\n",
    "We will define the state of our graph and the individual nodes (functions) that represent the steps in our agent's workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3c91ab4-6376-48d9-a10b-6204077ed923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph state and nodes defined.\n"
     ]
    }
   ],
   "source": [
    "# --- LangGraph Setup ---\n",
    "\n",
    "# 1. Define Graph State\n",
    "# This defines the object that is passed between nodes in the graph.\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: The user's question.\n",
    "        context: Retrieved context from the RAG system.\n",
    "        generation: The final generated answer from the LLM.\n",
    "        is_historical_query: A flag to determine if the query is historical.\n",
    "    \"\"\"\n",
    "    question: str\n",
    "    context: Annotated[List[str], operator.add] # Context will be accumulated\n",
    "    generation: str\n",
    "    is_historical_query: bool # New field to control flow\n",
    "\n",
    "# 2. Define Nodes (Functions)\n",
    "\n",
    "# Node 1: Query Classifier\n",
    "# This node determines if the incoming query is within the historical expert's scope.\n",
    "def query_classifier(state: GraphState):\n",
    "    \"\"\"\n",
    "    Determines if the incoming query is a historical question.\n",
    "    This helps in deciding whether to perform RAG or directly apply guardrails.\n",
    "    \"\"\"\n",
    "    print(\"---CLASSIFYING QUERY---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Use a simpler LLM call for classification to save tokens/latency\n",
    "    # Note: Using the same LLM for classification, but with a specific prompt.\n",
    "    classifier_prompt = ChatPromptTemplate.from_messages([\n",
    "        SystemMessage(content=\"You are a helpful assistant. Your task is to classify if a given user question is related to 'history', 'architecture', or 'engineering' of historical structures. Respond with 'YES' if it is, and 'NO' if it is not. Be strict with your classification. Examples of 'NO': current events, personal opinions, finance, medical advice, fictional scenarios.\"),\n",
    "        HumanMessage(content=f\"Is the following question historical/architectural/engineering-related? '{question}'\")\n",
    "    ])\n",
    "    classifier_chain = classifier_prompt | llm | StrOutputParser()\n",
    "\n",
    "    classification_result = classifier_chain.invoke({\"question\": question})\n",
    "    is_historical = \"YES\" in classification_result.upper()\n",
    "\n",
    "    print(f\"Query Classification: {classification_result.strip()} (Is Historical: {is_historical})\")\n",
    "    return {\"is_historical_query\": is_historical}\n",
    "\n",
    "\n",
    "# Node 2: Retrieve\n",
    "def retrieve(state: GraphState):\n",
    "    \"\"\"\n",
    "    Retrieves documents from the vector store based on the user's question.\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVING CONTEXT---\")\n",
    "    question = state[\"question\"]\n",
    "    docs = retriever.invoke(question)\n",
    "    context = [doc.page_content for doc in docs]\n",
    "    print(f\"Retrieved {len(context)} documents.\")\n",
    "    return {\"context\": context}\n",
    "\n",
    "# Node 3: Generate\n",
    "def generate(state: GraphState):\n",
    "    \"\"\"\n",
    "    Generates a response using the LLM, incorporating retrieved context if available,\n",
    "    and adhering to the system prompt with guardrails.\n",
    "    \"\"\"\n",
    "    print(\"---GENERATING RESPONSE---\")\n",
    "    question = state[\"question\"]\n",
    "    context = state[\"context\"]\n",
    "\n",
    "    # Load system prompt content from file\n",
    "    try:\n",
    "        with open(\"prompt.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "            system_prompt_content = file.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading prompt.txt in generate node: {e}\")\n",
    "        system_prompt_content = \"You are a helpful assistant.\" # Fallback\n",
    "\n",
    "    # --- FIX APPLIED HERE ---\n",
    "    # Construct the messages list directly for ChatGoogleGenerativeAI\n",
    "    # Using triple-quoted f-strings for multiline content to avoid backslash issues\n",
    "    messages = [SystemMessage(content=system_prompt_content)]\n",
    "\n",
    "    if context:\n",
    "        human_message_content = f\"\"\"Use the following retrieved context to answer the question. \\\n",
    "If the question cannot be answered from the provided context, state that you do not have sufficient information, \\\n",
    "but still adhere to your historical expert persona and guardrails.\n",
    "\n",
    "Context:\n",
    "{'   '.join(context)}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        messages.append(HumanMessage(content=human_message_content))\n",
    "    else:\n",
    "        human_message_content = f\"\"\"Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        messages.append(HumanMessage(content=human_message_content))\n",
    "\n",
    "    # Create the generation chain:\n",
    "    # Instead of `messages | llm | StrOutputParser()`,\n",
    "    # we directly invoke `llm` with the `messages` and then pipe to `StrOutputParser`.\n",
    "    rag_chain = (lambda x: llm.invoke(x[\"messages\"])) | StrOutputParser()\n",
    "\n",
    "    # Prepare input for the chain.\n",
    "    # The `messages` list is now passed as a dictionary key.\n",
    "    input_data = {\"messages\": messages}\n",
    "\n",
    "    generation_result = rag_chain.invoke(input_data)\n",
    "    print(\"Response generated.\")\n",
    "    return {\"generation\": generation_result}\n",
    "\n",
    "# 3. Define Conditional Edge\n",
    "def decide_to_retrieve(state: GraphState):\n",
    "    \"\"\"\n",
    "    Decides whether to retrieve context based on the query classification.\n",
    "    \"\"\"\n",
    "    print(\"---DECIDING TO RETRIEVE---\")\n",
    "    if state[\"is_historical_query\"]:\n",
    "        print(\"Decision: Query is historical, proceeding to retrieve.\")\n",
    "        return \"retrieve\"\n",
    "    else:\n",
    "        print(\"Decision: Query is not historical, skipping retrieval and directly generating (applying general guardrails).\")\n",
    "        return \"generate\" # Skip retrieval for non-historical questions\n",
    "\n",
    "print(\"Graph state and nodes defined.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c58a82e-33a6-4898-ae32-002daf9b97c5",
   "metadata": {},
   "source": [
    "---\n",
    "### 6. Build and Compile the LangGraph Workflow\n",
    "Now we assemble our nodes into a graph, defining the flow of execution based on the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a8ce4c2-6617-4f63-9ce5-f2508402d83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangGraph workflow compiled.\n"
     ]
    }
   ],
   "source": [
    "# --- Build the Graph ---\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"classify_query\", query_classifier)\n",
    "workflow.add_node(\"retrieve_context\", retrieve)\n",
    "workflow.add_node(\"generate_response\", generate)\n",
    "\n",
    "# Set entry point\n",
    "workflow.set_entry_point(\"classify_query\")\n",
    "\n",
    "# Add edges\n",
    "workflow.add_conditional_edges(\n",
    "    \"classify_query\",\n",
    "    decide_to_retrieve,\n",
    "    {\n",
    "        \"retrieve\": \"retrieve_context\",\n",
    "        \"generate\": \"generate_response\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# Add edge from retrieve to generate\n",
    "workflow.add_edge(\"retrieve_context\", \"generate_response\")\n",
    "\n",
    "# Set end point\n",
    "workflow.add_edge(\"generate_response\", END)\n",
    "\n",
    "# Compile the graph\n",
    "app = workflow.compile()\n",
    "\n",
    "print(\"LangGraph workflow compiled.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf018a28-db7b-4c0a-9a7e-91b370171bf2",
   "metadata": {},
   "source": [
    "---\n",
    "### 7. Run the Agent and Test Guardrails\n",
    "Let's test our RAG-enabled, stateful Historical Expert with both an on-topic and an off-topic question.\n",
    "\n",
    "#### Test Case 1: On-Topic Historical Question (RAG should activate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4c6b3d0-5f19-491a-bc6a-5a64b1cddba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test Case 1: Asking an on-topic historical question (RAG expected) ---\n",
      "User Question: Why does the Leaning Tower of Pisa lean, and what was done to fix it?\n",
      "---CLASSIFYING QUERY---\n",
      "Query Classification: YES (Is Historical: True)\n",
      "---DECIDING TO RETRIEVE---\n",
      "Decision: Query is historical, proceeding to retrieve.\n",
      "{'classify_query': {'is_historical_query': True}}\n",
      "---\n",
      "---RETRIEVING CONTEXT---\n",
      "Retrieved 4 documents.\n",
      "{'retrieve_context': {'context': ['Over the centuries, various efforts were made to correct or prevent the collapse of the tower. In 1838, architect Alessandro Gherardesca dug a pathway around the base to make the base visible, which caused the tower to lean even more. Benito Mussolini ordered that the tower be returned to a vertical position, and concrete was poured into the foundations in 1934, which also worsened the lean.\\n\\nThe most significant stabilization efforts took place from 1990 to 2001. An international committee of experts, led by Michele Jamiolkowski, undertook a major project. They used counterweights (600 tonnes of lead ingots) on the north side of the base and, more effectively, soil extraction. Soil was carefully removed from underneath the north side of the foundations, causing the tower to slowly settle back towards the north, reducing its tilt by about 45 cm (17.7 inches). This brought the lean back to what it was in 1838. The tower was reopened to the public in December 2001.', \"The Leaning Tower of Pisa is the campanile, or freestanding bell tower, of the cathedral of Pisa, Italy. It is situated behind the Pisa Cathedral and is the third oldest structure in Pisa's Cathedral Square (Piazza del Duomo), after the cathedral and the Pisa Baptistry.\\n\\nThe tower is known for its unintended tilt to one side, which began during construction in the 12th century due to soft ground that could not properly support the structure's weight. Construction began in August 1173. The tower was designed to be perfectly vertical, but it began to lean by the time the third floor was built in 1178. The soft soil, composed of clay, fine sand, and shells, was unstable. This initial lean was towards the north.\", 'Today, the Leaning Tower of Pisa is considered stable for at least another 200 years. Its unique tilt continues to attract millions of tourists worldwide.', 'Construction was halted for almost a century, which allowed the underlying soil to settle and compact, preventing the tower from toppling. When construction resumed in 1272, under Giovanni di Simone, the engineers tried to compensate for the tilt by building the upper floors with one side taller than the other. This caused the tower to start leaning in the opposite direction (south), creating a slight curve. Construction was again halted in 1284.\\n\\nThe seventh floor was completed in 1319, and the bell-chamber was finally added in 1372 by Tommaso di Andrea Pisano, who incorporated Gothic elements. The tower has 294 steps on the north side and 296 steps on the south side, as some steps had to be removed due to the lean.']}}\n",
      "---\n",
      "---GENERATING RESPONSE---\n",
      "Response generated.\n",
      "{'generate_response': {'generation': 'Certainly, I can explain why the Leaning Tower of Pisa leans and the efforts undertaken to stabilize it.\\n\\nThe Leaning Tower of Pisa leans primarily due to the unstable and soft ground upon which it was built. Construction began in August 1173, and by the time the third floor was completed in 1178, the tower had already begun to tilt. The underlying soil, composed of clay, fine sand, and shells, was simply not firm enough to support the immense weight of the structure uniformly. Initially, the lean was towards the north. When construction resumed in 1272, engineers attempted to compensate for this initial tilt by building the upper floors with one side taller than the other, inadvertently causing the tower to lean in the opposite direction (south) and creating a slight curve in its structure.\\n\\nOver the centuries, various attempts were made to correct or prevent the collapse of the tower:\\n\\n*   **1838:** Architect Alessandro Gherardesca dug a pathway around the base, which unfortunately caused the tower to lean even more.\\n*   **1934:** Benito Mussolini ordered concrete to be poured into the foundations to straighten the tower, an effort that also worsened the lean.\\n*   **1990-2001 (Most Significant Stabilization Efforts):** An international committee of experts, led by Michele Jamiolkowski, undertook a major project to stabilize the tower. Their methods included:\\n    *   **Counterweights:** 600 tonnes of lead ingots were placed on the north side of the base to provide a counterbalance.\\n    *   **Soil Extraction:** More effectively, soil was carefully removed from underneath the north side of the foundations. This process allowed the tower to slowly settle back towards the north, reducing its tilt by approximately 45 cm (17.7 inches) and bringing it back to the lean it had in 1838.\\n\\nThese comprehensive stabilization efforts were successful, and the tower was reopened to the public in December 2001, now considered stable for at least another 200 years.\\n\\nShould you have any more questions about historical structures or engineering feats, please feel free to ask.'}}\n",
      "---\n",
      "---CLASSIFYING QUERY---\n",
      "Query Classification: YES (Is Historical: True)\n",
      "---DECIDING TO RETRIEVE---\n",
      "Decision: Query is historical, proceeding to retrieve.\n",
      "---RETRIEVING CONTEXT---\n",
      "Retrieved 4 documents.\n",
      "---GENERATING RESPONSE---\n",
      "Response generated.\n",
      "\n",
      "Final Historical Expert's Response:\n",
      "Certainly, I can explain why the Leaning Tower of Pisa leans and the efforts made to stabilize it.\n",
      "\n",
      "The Leaning Tower of Pisa leans primarily due to the unstable and soft ground upon which it was built. Construction began in 1173, and by the time the third floor was completed in 1178, the tower had already started to tilt towards the north. This was because the underlying soil, composed of clay, fine sand, and shells, could not adequately support the immense weight of the structure. When construction resumed in 1272 after a century-long halt, engineers attempted to compensate for the lean by building the upper floors with one side taller than the other. This adjustment, however, caused the tower to begin leaning in the opposite direction, towards the south, resulting in a slight curve.\n",
      "\n",
      "Over the centuries, various attempts were made to correct its tilt:\n",
      "\n",
      "*   **1838:** Architect Alessandro Gherardesca dug a pathway around the base, which inadvertently caused the tower to lean even more.\n",
      "*   **1934:** Benito Mussolini ordered concrete to be poured into the foundations, an intervention that also worsened the lean.\n",
      "*   **1990-2001:** The most significant and successful stabilization efforts were undertaken by an international committee of experts led by Michele Jamiolkowski. Their project involved:\n",
      "    *   Placing 600 tonnes of lead ingots as counterweights on the north side of the base.\n",
      "    *   More effectively, carefully extracting soil from underneath the north side of the foundations. This process caused the tower to slowly settle back towards the north, reducing its tilt by approximately 45 cm (17.7 inches) to what it was in 1838.\n",
      "\n",
      "As a result of these efforts, the Leaning Tower of Pisa was reopened to the public in December 2001 and is currently considered stable for at least another 200 years.\n",
      "\n",
      "If you have more questions about historical architecture or engineering, please feel free to ask.\n"
     ]
    }
   ],
   "source": [
    "# Test Case 1: On-Topic Historical Question (RAG should activate)\n",
    "\n",
    "print(\"\\n--- Test Case 1: Asking an on-topic historical question (RAG expected) ---\")\n",
    "historical_question = \"Why does the Leaning Tower of Pisa lean, and what was done to fix it?\"\n",
    "print(f\"User Question: {historical_question}\")\n",
    "\n",
    "try:\n",
    "    inputs = {\"question\": historical_question, \"context\": [], \"generation\": \"\", \"is_historical_query\": False}\n",
    "    for s in app.stream(inputs):\n",
    "        print(s)\n",
    "        print(\"---\")\n",
    "    final_state = app.invoke(inputs)\n",
    "    print(\"\\nFinal Historical Expert's Response:\")\n",
    "    print(final_state[\"generation\"])\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during the historical question API call: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027c73a9-885a-49c0-8d39-0735f74ad8ee",
   "metadata": {},
   "source": [
    "---\n",
    "### Test Case 2: Off-Topic Question (Guardrails should activate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0056af7c-afbe-4ef9-ba6a-07c156a13aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test Case 2: Asking an off-topic question (Guardrails expected) ---\n",
      "User Question: Can you give me a detailed analysis of the current stock market trends for tech companies?\n",
      "---CLASSIFYING QUERY---\n",
      "Query Classification: NO (Is Historical: False)\n",
      "---DECIDING TO RETRIEVE---\n",
      "Decision: Query is not historical, skipping retrieval and directly generating (applying general guardrails).\n",
      "{'classify_query': {'is_historical_query': False}}\n",
      "---\n",
      "---GENERATING RESPONSE---\n",
      "Response generated.\n",
      "{'generate_response': {'generation': 'Thank you for your question. However, as a historical expert, my area of expertise lies in historical events, architectural marvels, and engineering feats of the past. I do not have the capacity to provide analysis on current stock market trends or contemporary financial information.\\n\\nIf you have any questions related to history, ancient architecture, or historical engineering, I would be happy to assist you.'}}\n",
      "---\n",
      "---CLASSIFYING QUERY---\n",
      "Query Classification: NO (Is Historical: False)\n",
      "---DECIDING TO RETRIEVE---\n",
      "Decision: Query is not historical, skipping retrieval and directly generating (applying general guardrails).\n",
      "---GENERATING RESPONSE---\n",
      "Response generated.\n",
      "\n",
      "Final Historical Expert's Response:\n",
      "As a historical expert, my knowledge base is focused on historical events, architectural marvels, and engineering feats from the past. I do not have access to real-time data or expertise in current financial markets or stock market trends for tech companies.\n",
      "\n",
      "If you have any questions related to historical topics, ancient architecture, or significant engineering achievements throughout history, I would be happy to assist you.\n"
     ]
    }
   ],
   "source": [
    "# Test Case 2: Off-Topic Question (Guardrails should activate)\n",
    "print(\"\\n--- Test Case 2: Asking an off-topic question (Guardrails expected) ---\")\n",
    "off_topic_question = \"Can you give me a detailed analysis of the current stock market trends for tech companies?\"\n",
    "print(f\"User Question: {off_topic_question}\")\n",
    "\n",
    "try:\n",
    "    inputs = {\"question\": off_topic_question, \"context\": [], \"generation\": \"\", \"is_historical_query\": False}\n",
    "    for s in app.stream(inputs):\n",
    "        print(s)\n",
    "        print(\"---\")\n",
    "    final_state = app.invoke(inputs)\n",
    "    print(\"\\nFinal Historical Expert's Response:\")\n",
    "    print(final_state[\"generation\"])\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during the off-topic question API call: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b189e587-bf86-4031-b345-62e20b86fd44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef8597f-4701-4306-bce3-3cb3e674b3be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-ai-2024.04-py310",
   "language": "python",
   "name": "conda-env-anaconda-ai-2024.04-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
