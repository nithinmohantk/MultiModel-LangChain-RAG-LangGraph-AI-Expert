{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89c716c1-6924-4820-8574-74b0e42fa963",
   "metadata": {},
   "source": [
    "## LangChain RAG Expert with LangGraph State Management (using Mistral AI)\n",
    "This Jupyter Notebook demonstrates how to build a sophisticated AI agent using LangChain and LangGraph, now integrated with Mistral AI API for the Large Language Model. The agent will act as a \"Helpful Historical Expert\" with the following key enhancements:\n",
    "\n",
    "Retrieval-Augmented Generation (RAG): The agent will query a local knowledge base (a text file) to retrieve relevant information before generating a response, ensuring factual accuracy and reducing hallucinations.\n",
    "\n",
    "LangGraph for State Management: We will use LangGraph to define a stateful workflow, allowing the agent to manage its internal state (e.g., current question, retrieved context) and execute steps like retrieval and response generation conditionally.\n",
    "\n",
    "Enhanced Guardrails: The detailed system prompt from prompt.txt will continue to guide the AI's persona, tone, and adherence to safety and scope constraints.\n",
    "\n",
    "Google Gemini Integration: The core language model for generation will be Google Gemini.\n",
    "\n",
    "We will test the system with both an on-topic historical question (which should leverage RAG) and an off-topic question (which should trigger the guardrails).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1286f8-dd54-4984-a37f-ec12db84b082",
   "metadata": {},
   "source": [
    "### 1. Setup and Installation\n",
    "First, we need to install all the necessary libraries. This includes LangChain components, langchain-mistralai for MistralAI integration, langchain-openai for embeddings (as MistralAI's native embeddings might require re-indexing the vector store, keeping OpenAI for consistency here), LangGraph for state management, FAISS for vector storage, and Tiktoken for tokenization.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7d0e8ea-0a5b-4568-bb89-fa0e05d1c4cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (0.3.26)\n",
      "Requirement already satisfied: langchain-mistralai in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (0.2.11)\n",
      "Requirement already satisfied: langchain-openai in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (0.3.28)\n",
      "Requirement already satisfied: langgraph in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (0.5.2)\n",
      "Requirement already satisfied: faiss-cpu in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (1.11.0)\n",
      "Requirement already satisfied: tiktoken in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from langchain) (0.3.68)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from langchain) (0.4.5)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from langchain) (2.11.7)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from langchain) (2.0.41)\n",
      "Requirement already satisfied: requests<3,>=2 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from langchain) (2.32.4)\n",
      "Requirement already satisfied: PyYAML>=5.3 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (4.14.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (2025.7.14)\n",
      "Requirement already satisfied: greenlet>=1 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
      "Requirement already satisfied: tokenizers<1,>=0.15.1 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from langchain-mistralai) (0.21.2)\n",
      "Requirement already satisfied: httpx<1,>=0.25.2 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from langchain-mistralai) (0.28.1)\n",
      "Requirement already satisfied: httpx-sse<1,>=0.3.1 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from langchain-mistralai) (0.4.1)\n",
      "Requirement already satisfied: anyio in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from httpx<1,>=0.25.2->langchain-mistralai) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from httpx<1,>=0.25.2->langchain-mistralai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.25.2->langchain-mistralai) (0.16.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from tokenizers<1,>=0.15.1->langchain-mistralai) (0.33.4)\n",
      "Requirement already satisfied: filelock in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain-mistralai) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain-mistralai) (2025.5.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain-mistralai) (4.67.1)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.86.0 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from langchain-openai) (1.95.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.1.0 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from langgraph) (2.1.0)\n",
      "Requirement already satisfied: langgraph-prebuilt<0.6.0,>=0.5.0 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from langgraph) (0.5.2)\n",
      "Requirement already satisfied: langgraph-sdk<0.2.0,>=0.1.42 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from langgraph) (0.1.72)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from langgraph) (3.5.0)\n",
      "Requirement already satisfied: ormsgpack>=1.10.0 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph) (1.10.0)\n",
      "Requirement already satisfied: orjson>=3.10.1 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10.18)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from faiss-cpu) (2.3.1)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: colorama in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain-mistralai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install necessary libraries\n",
    "%pip install -U langchain langchain-mistralai langchain-openai langgraph faiss-cpu tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "98cd0761-5437-4727-a214-7916d9778eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MISTRAL_API_KEY is set.\n",
      "OPENAI_API_KEY is set.\n",
      "Python executable: d:\\Workspace\\ai-dev\\MultiModel-LangChain-RAG-LangGraph-AI-Expert\\.venv\\Scripts\\python.exe\n",
      "LangChain, LangGraph, Mistral AI, and OpenAI setup complete.\n",
      "\n",
      "If you still encounter 'ModuleNotFoundError' after running this cell, please try:\n",
      "1. Restarting your Jupyter kernel (Kernel -> Restart Kernel...)\n",
      "2. Running this setup cell again.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from openai import OpenAI\n",
    "from langchain_mistralai import ChatMistralAI # Changed to ChatMistralAI for Mistral\n",
    "from langchain_openai import OpenAIEmbeddings # Still using OpenAI for embeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated, List\n",
    "import operator\n",
    "\n",
    "\n",
    "# --- Set your Mistral AI API Key ---\n",
    "# It's highly recommended to set this as an environment variable for security.\n",
    "# You can do this in your terminal before starting Jupyter:\n",
    "# export MISTRAL_API_KEY='your_mistral_api_key_here' (Linux/macOS)\n",
    "# $env:MISTRAL_API_KEY='your_mistral_api_key_here' (PowerShell)\n",
    "#\n",
    "# You will also need your OpenAI API key for embeddings:\n",
    "# export OPENAI_API_KEY='your_openai_api_key_here'\n",
    "#\n",
    "# If you must set them directly in the notebook (NOT recommended for production):\n",
    "# os.environ[\"MISTRAL_API_KEY\"] = \"YOUR_ACTUAL_MISTRAL_API_KEY\"\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_ACTUAL_OPENAI_API_KEY\"\n",
    "\n",
    "# Verify API keys are set\n",
    "if \"MISTRAL_API_KEY\" not in os.environ:\n",
    "    print(\"WARNING: MISTRAL_API_KEY environment variable not set.\")\n",
    "    print(\"Please set it before proceeding, or uncomment the line above to set it directly (not recommended).\")\n",
    "else:\n",
    "    print(\"MISTRAL_API_KEY is set.\")\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    print(\"WARNING: OPENAI_API_KEY environment variable not set (needed for embeddings).\")\n",
    "    print(\"Please set it before proceeding, or uncomment the line above to set it directly (not recommended).\")\n",
    "else:\n",
    "    print(\"OPENAI_API_KEY is set.\")\n",
    "\n",
    "\n",
    "# Print the Python executable path to help debug environment issues\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "\n",
    "# Initialize the ChatMistralAI model for generation\n",
    "# Using mistral-large. You might choose other models available via Mistral AI.\n",
    "llm = ChatMistralAI(model=\"mistral-medium\", temperature=0.7) # Adjust temperature for creativity (0.0 for deterministic)\n",
    "\n",
    "# Initialize OpenAIEmbeddings for RAG (keeping consistent with previous notebooks)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "print(\"LangChain, LangGraph, Mistral AI, and OpenAI setup complete.\")\n",
    "print(\"\\nIf you still encounter 'ModuleNotFoundError' after running this cell, please try:\")\n",
    "print(\"1. Restarting your Jupyter kernel (Kernel -> Restart Kernel...)\")\n",
    "print(\"2. Running this setup cell again.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aabcec8-c838-4f1a-b258-31c520a8c29d",
   "metadata": {},
   "source": [
    "### 2. Create the prompt.txt File\n",
    "Create or update a file named prompt.txt in the same directory as this Jupyter Notebook. This file will contain the detailed system prompt with all the guardrails for your Historical Expert.\n",
    "\n",
    "prompt.txt content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28e45fe-3527-4e8f-a9bc-0072be8af0e0",
   "metadata": {},
   "source": [
    "### 3. Create Knowledge Base File (pisa_history.txt)\n",
    "Create a new file named pisa_history.txt in the same directory as this Jupyter Notebook. This file will serve as our knowledge base for RAG.\n",
    "\n",
    "pisa_history.txt content (example, feel free to expand):\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7e23c0-1fa4-4749-8379-0c0d7c72069a",
   "metadata": {},
   "source": [
    "### 4. RAG Setup: Create Retriever\n",
    "Here, we'll load our pisa_history.txt file, split it into manageable chunks, create embeddings for these chunks, and then store them in a FAISS vector store to enable efficient retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cf1251a3-beb2-48fc-8332-4f97cd50f159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded RAG document from 'pisa_history.txt'\n",
      "Split document into 4 chunks.\n",
      "FAISS vector store created.\n",
      "Retriever created.\n"
     ]
    }
   ],
   "source": [
    "# --- RAG Setup ---\n",
    "rag_file_path = \"pisa_history.txt\"\n",
    "\n",
    "# 1. Load the document\n",
    "try:\n",
    "    loader = TextLoader(rag_file_path, encoding=\"utf-8\")\n",
    "    documents = loader.load()\n",
    "    print(f\"Successfully loaded RAG document from '{rag_file_path}'\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The RAG file '{rag_file_path}' was not found. Please create it.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the RAG document: {e}\")\n",
    "    exit()\n",
    "\n",
    "# 2. Split the document into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(documents)\n",
    "print(f\"Split document into {len(splits)} chunks.\")\n",
    "\n",
    "# 3. Create a FAISS vector store from the chunks and embeddings\n",
    "vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings)\n",
    "print(\"FAISS vector store created.\")\n",
    "\n",
    "# 4. Create a retriever\n",
    "retriever = vectorstore.as_retriever()\n",
    "print(\"Retriever created.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525a4ce6-4a15-41e8-8981-a3e59cc1d93b",
   "metadata": {},
   "source": [
    "---\n",
    "### 5. LangGraph Setup: Define Graph State and Nodes\n",
    "We will define the state of our graph and the individual nodes (functions) that represent the steps in our agent's workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c3c91ab4-6376-48d9-a10b-6204077ed923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph state and nodes defined.\n"
     ]
    }
   ],
   "source": [
    "# --- LangGraph Setup ---\n",
    "\n",
    "# 1. Define Graph State\n",
    "# This defines the object that is passed between nodes in the graph.\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: The user's question.\n",
    "        context: Retrieved context from the RAG system.\n",
    "        generation: The final generated answer from the LLM.\n",
    "        is_historical_query: A flag to determine if the query is historical.\n",
    "    \"\"\"\n",
    "    question: str\n",
    "    context: Annotated[List[str], operator.add] # Context will be accumulated\n",
    "    generation: str\n",
    "    is_historical_query: bool # New field to control flow\n",
    "\n",
    "# 2. Define Nodes (Functions)\n",
    "\n",
    "# Node 1: Query Classifier\n",
    "# This node determines if the incoming query is within the historical expert's scope.\n",
    "def query_classifier(state: GraphState):\n",
    "    \"\"\"\n",
    "    Determines if the incoming query is a historical question.\n",
    "    This helps in deciding whether to perform RAG or directly apply guardrails.\n",
    "    \"\"\"\n",
    "    print(\"---CLASSIFYING QUERY---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Use a simpler LLM call for classification to save tokens/latency\n",
    "    # Note: Using the same LLM for classification, but with a specific prompt.\n",
    "    classifier_prompt = ChatPromptTemplate.from_messages([\n",
    "        SystemMessage(content=\"You are a helpful assistant. Your task is to classify if a given user question is related to 'history', 'architecture', or 'engineering' of historical structures. Respond with 'YES' if it is, and 'NO' if it is not. Be strict with your classification. Examples of 'NO': current events, personal opinions, finance, medical advice, fictional scenarios.\"),\n",
    "        HumanMessage(content=f\"Is the following question historical/architectural/engineering-related? '{question}'\")\n",
    "    ])\n",
    "    classifier_chain = classifier_prompt | llm | StrOutputParser()\n",
    "\n",
    "    classification_result = classifier_chain.invoke({\"question\": question})\n",
    "    is_historical = \"YES\" in classification_result.upper()\n",
    "\n",
    "    print(f\"Query Classification: {classification_result.strip()} (Is Historical: {is_historical})\")\n",
    "    return {\"is_historical_query\": is_historical}\n",
    "\n",
    "\n",
    "# Node 2: Retrieve\n",
    "def retrieve(state: GraphState):\n",
    "    \"\"\"\n",
    "    Retrieves documents from the vector store based on the user's question.\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVING CONTEXT---\")\n",
    "    question = state[\"question\"]\n",
    "    docs = retriever.invoke(question)\n",
    "    context = [doc.page_content for doc in docs]\n",
    "    print(f\"Retrieved {len(context)} documents.\")\n",
    "    return {\"context\": context}\n",
    "\n",
    "# Node 3: Generate\n",
    "def generate(state: GraphState):\n",
    "    \"\"\"\n",
    "    Generates a response using the LLM, incorporating retrieved context if available,\n",
    "    and adhering to the system prompt with guardrails.\n",
    "    \"\"\"\n",
    "    print(\"---GENERATING RESPONSE---\")\n",
    "    question = state[\"question\"]\n",
    "    context = state[\"context\"]\n",
    "\n",
    "    # Load system prompt content from file\n",
    "    try:\n",
    "        with open(\"prompt.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "            system_prompt_content = file.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading prompt.txt in generate node: {e}\")\n",
    "        system_prompt_content = \"You are a helpful assistant.\" # Fallback\n",
    "\n",
    "    # --- FIX APPLIED HERE ---\n",
    "    # Construct the messages list directly for ChatGoogleGenerativeAI\n",
    "    # Using triple-quoted f-strings for multiline content to avoid backslash issues\n",
    "    messages = [SystemMessage(content=system_prompt_content)]\n",
    "\n",
    "    if context:\n",
    "        human_message_content = f\"\"\"Use the following retrieved context to answer the question. \\\n",
    "If the question cannot be answered from the provided context, state that you do not have sufficient information, \\\n",
    "but still adhere to your historical expert persona and guardrails.\n",
    "\n",
    "Context:\n",
    "{'   '.join(context)}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        messages.append(HumanMessage(content=human_message_content))\n",
    "    else:\n",
    "        human_message_content = f\"\"\"Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        messages.append(HumanMessage(content=human_message_content))\n",
    "\n",
    "    # Create the generation chain:\n",
    "    # Instead of `messages | llm | StrOutputParser()`,\n",
    "    # we directly invoke `llm` with the `messages` and then pipe to `StrOutputParser`.\n",
    "    rag_chain = (lambda x: llm.invoke(x[\"messages\"])) | StrOutputParser()\n",
    "\n",
    "    # Prepare input for the chain.\n",
    "    # The `messages` list is now passed as a dictionary key.\n",
    "    input_data = {\"messages\": messages}\n",
    "\n",
    "    generation_result = rag_chain.invoke(input_data)\n",
    "    print(\"Response generated.\")\n",
    "    return {\"generation\": generation_result}\n",
    "\n",
    "# 3. Define Conditional Edge\n",
    "def decide_to_retrieve(state: GraphState):\n",
    "    \"\"\"\n",
    "    Decides whether to retrieve context based on the query classification.\n",
    "    \"\"\"\n",
    "    print(\"---DECIDING TO RETRIEVE---\")\n",
    "    if state[\"is_historical_query\"]:\n",
    "        print(\"Decision: Query is historical, proceeding to retrieve.\")\n",
    "        return \"retrieve\"\n",
    "    else:\n",
    "        print(\"Decision: Query is not historical, skipping retrieval and directly generating (applying general guardrails).\")\n",
    "        return \"generate\" # Skip retrieval for non-historical questions\n",
    "\n",
    "print(\"Graph state and nodes defined.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c58a82e-33a6-4898-ae32-002daf9b97c5",
   "metadata": {},
   "source": [
    "---\n",
    "### 6. Build and Compile the LangGraph Workflow\n",
    "Now we assemble our nodes into a graph, defining the flow of execution based on the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0a8ce4c2-6617-4f63-9ce5-f2508402d83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangGraph workflow compiled.\n"
     ]
    }
   ],
   "source": [
    "# --- Build the Graph ---\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"classify_query\", query_classifier)\n",
    "workflow.add_node(\"retrieve_context\", retrieve)\n",
    "workflow.add_node(\"generate_response\", generate)\n",
    "\n",
    "# Set entry point\n",
    "workflow.set_entry_point(\"classify_query\")\n",
    "\n",
    "# Add edges\n",
    "workflow.add_conditional_edges(\n",
    "    \"classify_query\",\n",
    "    decide_to_retrieve,\n",
    "    {\n",
    "        \"retrieve\": \"retrieve_context\",\n",
    "        \"generate\": \"generate_response\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# Add edge from retrieve to generate\n",
    "workflow.add_edge(\"retrieve_context\", \"generate_response\")\n",
    "\n",
    "# Set end point\n",
    "workflow.add_edge(\"generate_response\", END)\n",
    "\n",
    "# Compile the graph\n",
    "app = workflow.compile()\n",
    "\n",
    "print(\"LangGraph workflow compiled.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf018a28-db7b-4c0a-9a7e-91b370171bf2",
   "metadata": {},
   "source": [
    "---\n",
    "### 7. Run the Agent and Test Guardrails\n",
    "Let's test our RAG-enabled, stateful Historical Expert with both an on-topic and an off-topic question.\n",
    "\n",
    "#### Test Case 1: On-Topic Historical Question (RAG should activate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d4c6b3d0-5f19-491a-bc6a-5a64b1cddba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test Case 1: Asking an on-topic historical question (RAG expected) ---\n",
      "User Question: Why does the Leaning Tower of Pisa lean, and what was done to fix it?\n",
      "---CLASSIFYING QUERY---\n",
      "Query Classification: YES (Is Historical: True)\n",
      "---DECIDING TO RETRIEVE---\n",
      "Decision: Query is historical, proceeding to retrieve.\n",
      "{'classify_query': {'is_historical_query': True}}\n",
      "---\n",
      "---RETRIEVING CONTEXT---\n",
      "Retrieved 4 documents.\n",
      "{'retrieve_context': {'context': ['Over the centuries, various efforts were made to correct or prevent the collapse of the tower. In 1838, architect Alessandro Gherardesca dug a pathway around the base to make the base visible, which caused the tower to lean even more. Benito Mussolini ordered that the tower be returned to a vertical position, and concrete was poured into the foundations in 1934, which also worsened the lean.\\n\\nThe most significant stabilization efforts took place from 1990 to 2001. An international committee of experts, led by Michele Jamiolkowski, undertook a major project. They used counterweights (600 tonnes of lead ingots) on the north side of the base and, more effectively, soil extraction. Soil was carefully removed from underneath the north side of the foundations, causing the tower to slowly settle back towards the north, reducing its tilt by about 45 cm (17.7 inches). This brought the lean back to what it was in 1838. The tower was reopened to the public in December 2001.', \"The Leaning Tower of Pisa is the campanile, or freestanding bell tower, of the cathedral of Pisa, Italy. It is situated behind the Pisa Cathedral and is the third oldest structure in Pisa's Cathedral Square (Piazza del Duomo), after the cathedral and the Pisa Baptistry.\\n\\nThe tower is known for its unintended tilt to one side, which began during construction in the 12th century due to soft ground that could not properly support the structure's weight. Construction began in August 1173. The tower was designed to be perfectly vertical, but it began to lean by the time the third floor was built in 1178. The soft soil, composed of clay, fine sand, and shells, was unstable. This initial lean was towards the north.\", 'Today, the Leaning Tower of Pisa is considered stable for at least another 200 years. Its unique tilt continues to attract millions of tourists worldwide.', 'Construction was halted for almost a century, which allowed the underlying soil to settle and compact, preventing the tower from toppling. When construction resumed in 1272, under Giovanni di Simone, the engineers tried to compensate for the tilt by building the upper floors with one side taller than the other. This caused the tower to start leaning in the opposite direction (south), creating a slight curve. Construction was again halted in 1284.\\n\\nThe seventh floor was completed in 1319, and the bell-chamber was finally added in 1372 by Tommaso di Andrea Pisano, who incorporated Gothic elements. The tower has 294 steps on the north side and 296 steps on the south side, as some steps had to be removed due to the lean.']}}\n",
      "---\n",
      "---GENERATING RESPONSE---\n",
      "Response generated.\n",
      "{'generate_response': {'generation': \"The Leaning Tower of Pisa is renowned for its unintended tilt, which has fascinated people for centuries. The tower began to lean during its construction in the 12th century due to the soft ground beneath it, which was unable to support the structure's weight adequately. The soil, composed of clay, fine sand, and shells, was unstable, causing the tower to lean towards the north initially.\\n\\nOver the centuries, various efforts were made to correct or prevent the collapse of the tower. Some notable attempts include:\\n\\n- In 1838, architect Alessandro Gherardesca dug a pathway around the base to make it visible, which unfortunately caused the tower to lean even more.\\n- In 1934, under Benito Mussolini's orders, concrete was poured into the foundations to return the tower to a vertical position, but this also worsened the lean.\\n\\nThe most significant and successful stabilization efforts took place from 1990 to 2001. An international committee of experts, led by Michele Jamiolkowski, undertook a major project that included:\\n\\n- Using counterweights (600 tonnes of lead ingots) on the north side of the base.\\n- Carefully removing soil from underneath the north side of the foundations, which caused the tower to slowly settle back towards the north.\\n- Reducing the tilt by about 45 cm (17.7 inches), bringing the lean back to what it was in 1838.\\n\\nThese efforts have made the Leaning Tower of Pisa stable for at least another 200 years, ensuring that its unique tilt continues to attract millions of tourists worldwide.\\n\\nIf you have any more questions about historical structures or events, feel free to ask!\"}}\n",
      "---\n",
      "---CLASSIFYING QUERY---\n",
      "Query Classification: YES (Is Historical: True)\n",
      "---DECIDING TO RETRIEVE---\n",
      "Decision: Query is historical, proceeding to retrieve.\n",
      "---RETRIEVING CONTEXT---\n",
      "Retrieved 4 documents.\n",
      "---GENERATING RESPONSE---\n",
      "Response generated.\n",
      "\n",
      "Final Historical Expert's Response:\n",
      "The Leaning Tower of Pisa is renowned for its unintended tilt, which is primarily due to the unstable foundation soil. Hereâ€™s a detailed explanation of why it leans and the measures taken to stabilize it:\n",
      "\n",
      "### Why the Leaning Tower of Pisa Leans\n",
      "\n",
      "- **Construction and Soil Issues:** Construction of the tower began in August 1173. The tower was designed to be perfectly vertical, but it started to lean by the time the third floor was completed in 1178.\n",
      "- **Soft Ground:** The soil beneath the tower is composed of soft clay, fine sand, and shells, which could not adequately support the structure's weight. This instability caused the tower to begin tilting to the north initially.\n",
      "- **Construction Halts:** Construction was halted for nearly a century, allowing the soil to settle and compact. When construction resumed in 1272, engineers attempted to correct the tilt by building the upper floors with one side taller than the other. This adjustment caused the tower to lean in the opposite direction (south), creating a slight curve in the structure.\n",
      "\n",
      "### Efforts to Stabilize the Tower\n",
      "\n",
      "- **Early Attempts:**\n",
      "  - In 1838, architect Alessandro Gherardesca dug a pathway around the base to expose the foundation, which inadvertently caused the tower to lean even more.\n",
      "  - In 1934, Benito Mussolini ordered the tower to be straightened by pouring concrete into the foundations, which also worsened the lean.\n",
      "\n",
      "- **Modern Stabilization (1990-2001):**\n",
      "  - An international committee of experts, led by Michele Jamiolkowski, undertook a significant stabilization project.\n",
      "  - **Counterweights:** Initially, 600 tonnes of lead ingots were placed on the north side of the base to counteract the lean.\n",
      "  - **Soil Extraction:** More effectively, soil was carefully removed from underneath the north side of the foundations. This caused the tower to slowly settle back towards the north, reducing its tilt by about 45 cm (17.7 inches). This brought the lean back to what it was in 1838.\n",
      "  - The tower was reopened to the public in December 2001 and is now considered stable for at least another 200 years.\n",
      "\n",
      "The Leaning Tower of Pisa remains a marvel of medieval engineering and a testament to the ongoing efforts to preserve historical structures. Its unique tilt continues to fascinate and attract millions of tourists worldwide. If you have any further questions about historical structures or engineering feats, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "# Test Case 1: On-Topic Historical Question (RAG should activate)\n",
    "\n",
    "print(\"\\n--- Test Case 1: Asking an on-topic historical question (RAG expected) ---\")\n",
    "historical_question = \"Why does the Leaning Tower of Pisa lean, and what was done to fix it?\"\n",
    "print(f\"User Question: {historical_question}\")\n",
    "\n",
    "try:\n",
    "    inputs = {\"question\": historical_question, \"context\": [], \"generation\": \"\", \"is_historical_query\": False}\n",
    "    for s in app.stream(inputs):\n",
    "        print(s)\n",
    "        print(\"---\")\n",
    "    final_state = app.invoke(inputs)\n",
    "    print(\"\\nFinal Historical Expert's Response:\")\n",
    "    print(final_state[\"generation\"])\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during the historical question API call: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027c73a9-885a-49c0-8d39-0735f74ad8ee",
   "metadata": {},
   "source": [
    "---\n",
    "### Test Case 2: Off-Topic Question (Guardrails should activate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0056af7c-afbe-4ef9-ba6a-07c156a13aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test Case 2: Asking an off-topic question (Guardrails expected) ---\n",
      "User Question: Can you give me a detailed analysis of the current stock market trends for tech companies?\n",
      "---CLASSIFYING QUERY---\n",
      "Query Classification: NO (Is Historical: False)\n",
      "---DECIDING TO RETRIEVE---\n",
      "Decision: Query is not historical, skipping retrieval and directly generating (applying general guardrails).\n",
      "{'classify_query': {'is_historical_query': False}}\n",
      "---\n",
      "---GENERATING RESPONSE---\n",
      "Response generated.\n",
      "{'generate_response': {'generation': \"I'm sorry, but I can only provide information related to history, architecture, and engineering. I cannot offer advice or analysis on current events, including stock market trends. Is there a historical event or architectural marvel you'd like to learn more about? I'd be happy to help with that!\"}}\n",
      "---\n",
      "---CLASSIFYING QUERY---\n",
      "Query Classification: NO (Is Historical: False)\n",
      "---DECIDING TO RETRIEVE---\n",
      "Decision: Query is not historical, skipping retrieval and directly generating (applying general guardrails).\n",
      "---GENERATING RESPONSE---\n",
      "Response generated.\n",
      "\n",
      "Final Historical Expert's Response:\n",
      "I'm sorry, but I can't provide information on current stock market trends as my expertise is focused on historical events, architectural marvels, and engineering feats. I don't have real-time data or the ability to analyze current market trends. Is there a historical event or an architectural marvel you'd like to learn about instead? I'm here to help with those topics!\n"
     ]
    }
   ],
   "source": [
    "# Test Case 2: Off-Topic Question (Guardrails should activate)\n",
    "print(\"\\n--- Test Case 2: Asking an off-topic question (Guardrails expected) ---\")\n",
    "off_topic_question = \"Can you give me a detailed analysis of the current stock market trends for tech companies?\"\n",
    "print(f\"User Question: {off_topic_question}\")\n",
    "\n",
    "try:\n",
    "    inputs = {\"question\": off_topic_question, \"context\": [], \"generation\": \"\", \"is_historical_query\": False}\n",
    "    for s in app.stream(inputs):\n",
    "        print(s)\n",
    "        print(\"---\")\n",
    "    final_state = app.invoke(inputs)\n",
    "    print(\"\\nFinal Historical Expert's Response:\")\n",
    "    print(final_state[\"generation\"])\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during the off-topic question API call: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b189e587-bf86-4031-b345-62e20b86fd44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef8597f-4701-4306-bce3-3cb3e674b3be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
