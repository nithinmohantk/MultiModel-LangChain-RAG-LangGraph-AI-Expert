{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89c716c1-6924-4820-8574-74b0e42fa963",
   "metadata": {},
   "source": [
    "## LangChain RAG Expert with LangGraph State Management (using Nvidia AI Endpoints)\n",
    "This Jupyter Notebook demonstrates how to build a sophisticated AI agent using LangChain and LangGraph, now integrated with NVIDIA AI Endpoints API for the Large Language Model. The agent will act as a \"Helpful Historical Expert\" with the following key enhancements:\n",
    "\n",
    "- **Retrieval-Augmented Generation (RAG):** The agent will query a local knowledge base (a text file) to retrieve relevant information before generating a response, ensuring factual accuracy and reducing hallucinations.\n",
    "\n",
    "- **LangGraph for State Management:** We will use LangGraph to define a stateful workflow, allowing the agent to manage its internal state (e.g., current question, retrieved context) and execute steps like retrieval and response generation conditionally.\n",
    "\n",
    "-  **Guardrails:** The detailed system prompt from prompt.txt will continue to guide the AI's persona, tone, and adherence to safety and scope constraints.\n",
    "\n",
    "- **Google Gemini Integration:** The core language model for generation will be Google Gemini.\n",
    "\n",
    "We will test the system with both an on-topic historical question (which should leverage RAG) and an off-topic question (which should trigger the guardrails).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1286f8-dd54-4984-a37f-ec12db84b082",
   "metadata": {},
   "source": [
    "### 1. Setup and Installation\n",
    "First, we need to install all the necessary libraries. This includes LangChain components, langchain-mistralai for MistralAI integration, langchain-openai for embeddings (as MistralAI's native embeddings might require re-indexing the vector store, keeping OpenAI for consistency here), LangGraph for state management, FAISS for vector storage, and Tiktoken for tokenization.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7d0e8ea-0a5b-4568-bb89-fa0e05d1c4cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (0.3.26)\n",
      "Requirement already satisfied: langchain-community in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (0.3.27)\n",
      "Requirement already satisfied: langchain-nvidia-ai-endpoints in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (0.3.10)\n",
      "Requirement already satisfied: langchain-openai in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (0.3.28)\n",
      "Requirement already satisfied: langgraph in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (0.5.2)\n",
      "Collecting langgraph\n",
      "  Downloading langgraph-0.5.3-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: faiss-cpu in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (1.11.0)\n",
      "Requirement already satisfied: tiktoken in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from langchain) (0.3.68)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from langchain) (0.4.5)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from langchain) (2.11.7)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from langchain) (2.0.41)\n",
      "Requirement already satisfied: requests<3,>=2 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from langchain) (2.32.4)\n",
      "Requirement already satisfied: PyYAML>=5.3 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (4.14.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (2025.7.14)\n",
      "Requirement already satisfied: greenlet>=1 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from langchain-community) (3.12.14)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from langchain-community) (2.10.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from langchain-community) (0.4.1)\n",
      "Requirement already satisfied: numpy>=2.1.0 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from langchain-community) (2.3.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from langchain-nvidia-ai-endpoints) (1.2.0)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.86.0 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from langchain-openai) (1.95.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.86.0->langchain-openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.86.0->langchain-openai) (0.16.0)\n",
      "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.1.0 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from langgraph) (2.1.0)\n",
      "Requirement already satisfied: langgraph-prebuilt<0.6.0,>=0.5.0 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from langgraph) (0.5.2)\n",
      "Requirement already satisfied: langgraph-sdk<0.2.0,>=0.1.42 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from langgraph) (0.1.72)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from langgraph) (3.5.0)\n",
      "Requirement already satisfied: ormsgpack>=1.10.0 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph) (1.10.0)\n",
      "Requirement already satisfied: orjson>=3.10.1 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: colorama in d:\\workspace\\ai-dev\\multimodel-langchain-rag-langgraph-ai-expert\\.venv\\lib\\site-packages (from tqdm>4->openai<2.0.0,>=1.86.0->langchain-openai) (0.4.6)\n",
      "Downloading langgraph-0.5.3-py3-none-any.whl (143 kB)\n",
      "Installing collected packages: langgraph\n",
      "  Attempting uninstall: langgraph\n",
      "    Found existing installation: langgraph 0.5.2\n",
      "    Uninstalling langgraph-0.5.2:\n",
      "      Successfully uninstalled langgraph-0.5.2\n",
      "Successfully installed langgraph-0.5.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install necessary libraries\n",
    "%pip install -U langchain langchain-community langchain-nvidia-ai-endpoints langchain-openai langgraph faiss-cpu tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cd0761-5437-4727-a214-7916d9778eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA_API_KEY is set.\n",
      "OPENAI_API_KEY is set.\n",
      "Python executable: d:\\Workspace\\ai-dev\\MultiModel-LangChain-RAG-LangGraph-AI-Expert\\.venv\\Scripts\\python.exe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Workspace\\ai-dev\\MultiModel-LangChain-RAG-LangGraph-AI-Expert\\.venv\\Lib\\site-packages\\langchain_nvidia_ai_endpoints\\_common.py:217: UserWarning: Found mistralai/magistral-small-2506 in available_models, but type is unknown and inference may fail.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain, LangGraph, NVIDIA Chat, and OpenAI setup complete.\n",
      "\n",
      "If you still encounter 'ModuleNotFoundError' after running this cell, please try:\n",
      "1. Restarting your Jupyter kernel (Kernel -> Restart Kernel...)\n",
      "2. Running this setup cell again.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from openai import OpenAI\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA # Changed to ChatNVIDIA for NVIDIA Chat\n",
    "from langchain_openai import OpenAIEmbeddings # Still using OpenAI for embeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated, List\n",
    "import operator\n",
    "\n",
    "\n",
    "# --- Set your NVIDIA API Key ---\n",
    "# It's highly recommended to set this as an environment variable for security.\n",
    "# You can do this in your terminal before starting Jupyter:\n",
    "# export NVIDIA_API_KEY='your_nvidia_api_key_here' (Linux/macOS)\n",
    "# $env:NVIDIA_API_KEY='your_nvidia_api_key_here' (PowerShell)\n",
    "#\n",
    "# You will also need your OpenAI API key for embeddings:\n",
    "# export OPENAI_API_KEY='your_openai_api_key_here'\n",
    "#\n",
    "# If you must set them directly in the notebook (NOT recommended for production):\n",
    "# os.environ[\"NVIDIA_API_KEY\"] = \"YOUR_ACTUAL_NVIDIA_API_KEY\"\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_ACTUAL_OPENAI_API_KEY\"\n",
    "\n",
    "# Verify API keys are set\n",
    "if \"NVIDIA_API_KEY\" not in os.environ:\n",
    "    print(\"WARNING: NVIDIA_API_KEY environment variable not set.\")\n",
    "    print(\"Please set it before proceeding, or uncomment the line above to set it directly (not recommended).\")\n",
    "else:\n",
    "    print(\"NVIDIA_API_KEY is set.\")\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    print(\"WARNING: OPENAI_API_KEY environment variable not set (needed for embeddings).\")\n",
    "    print(\"Please set it before proceeding, or uncomment the line above to set it directly (not recommended).\")\n",
    "else:\n",
    "    print(\"OPENAI_API_KEY is set.\")\n",
    "\n",
    "\n",
    "# Print the Python executable path to help debug environment issues\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "\n",
    "# Initialize the ChatNVIDIA model for generation\n",
    "# Using nvidia/nemotron-4-340b-instruct. You can find other models at NVIDIA AI Playground.\n",
    "# model_name = \"meta/llama-3.3-70b-instruct\"  # Adjust model name as needed\n",
    "# model_name = \"deepseek-ai/deepseek-r1\"  # Adjust model name as \n",
    "model_name = \"mistralai/magistral-small-2506\" # Adjust model name as needed\n",
    "\n",
    "# You might choose other models available via NVIDIA AI Endpoints.\n",
    "llm = ChatNVIDIA(model=model_name, temperature=0.7) # Adjust temperature for creativity (0.0 for deterministic)\n",
    "\n",
    "# Initialize OpenAIEmbeddings for RAG (keeping consistent with previous notebooks)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "print(\"LangChain, LangGraph, NVIDIA Chat, and OpenAI setup complete.\")\n",
    "print(\"\\nIf you still encounter 'ModuleNotFoundError' after running this cell, please try:\")\n",
    "print(\"1. Restarting your Jupyter kernel (Kernel -> Restart Kernel...)\")\n",
    "print(\"2. Running this setup cell again.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aabcec8-c838-4f1a-b258-31c520a8c29d",
   "metadata": {},
   "source": [
    "### 2. Create the prompt.txt File\n",
    "Create or update a file named prompt.txt in the same directory as this Jupyter Notebook. This file will contain the detailed system prompt with all the guardrails for your Historical Expert.\n",
    "\n",
    "prompt.txt content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28e45fe-3527-4e8f-a9bc-0072be8af0e0",
   "metadata": {},
   "source": [
    "### 3. Create Knowledge Base File (pisa_history.txt)\n",
    "Create a new file named pisa_history.txt in the same directory as this Jupyter Notebook. This file will serve as our knowledge base for RAG.\n",
    "\n",
    "pisa_history.txt content (example, feel free to expand):\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7e23c0-1fa4-4749-8379-0c0d7c72069a",
   "metadata": {},
   "source": [
    "### 4. RAG Setup: Create Retriever\n",
    "Here, we'll load our pisa_history.txt file, split it into manageable chunks, create embeddings for these chunks, and then store them in a FAISS vector store to enable efficient retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf1251a3-beb2-48fc-8332-4f97cd50f159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded RAG document from 'pisa_history.txt'\n",
      "Split document into 4 chunks.\n",
      "FAISS vector store created.\n",
      "Retriever created.\n"
     ]
    }
   ],
   "source": [
    "# --- RAG Setup ---\n",
    "rag_file_path = \"pisa_history.txt\"\n",
    "\n",
    "# 1. Load the document\n",
    "try:\n",
    "    loader = TextLoader(rag_file_path, encoding=\"utf-8\")\n",
    "    documents = loader.load()\n",
    "    print(f\"Successfully loaded RAG document from '{rag_file_path}'\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The RAG file '{rag_file_path}' was not found. Please create it.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the RAG document: {e}\")\n",
    "    exit()\n",
    "\n",
    "# 2. Split the document into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(documents)\n",
    "print(f\"Split document into {len(splits)} chunks.\")\n",
    "\n",
    "# 3. Create a FAISS vector store from the chunks and embeddings\n",
    "vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings)\n",
    "print(\"FAISS vector store created.\")\n",
    "\n",
    "# 4. Create a retriever\n",
    "retriever = vectorstore.as_retriever()\n",
    "print(\"Retriever created.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525a4ce6-4a15-41e8-8981-a3e59cc1d93b",
   "metadata": {},
   "source": [
    "---\n",
    "### 5. LangGraph Setup: Define Graph State and Nodes\n",
    "We will define the state of our graph and the individual nodes (functions) that represent the steps in our agent's workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3c91ab4-6376-48d9-a10b-6204077ed923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph state and nodes defined.\n"
     ]
    }
   ],
   "source": [
    "# --- LangGraph Setup ---\n",
    "\n",
    "# 1. Define Graph State\n",
    "# This defines the object that is passed between nodes in the graph.\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: The user's question.\n",
    "        context: Retrieved context from the RAG system.\n",
    "        generation: The final generated answer from the LLM.\n",
    "        is_historical_query: A flag to determine if the query is historical.\n",
    "    \"\"\"\n",
    "    question: str\n",
    "    context: Annotated[List[str], operator.add] # Context will be accumulated\n",
    "    generation: str\n",
    "    is_historical_query: bool # New field to control flow\n",
    "\n",
    "# 2. Define Nodes (Functions)\n",
    "\n",
    "# Node 1: Query Classifier\n",
    "# This node determines if the incoming query is within the historical expert's scope.\n",
    "def query_classifier(state: GraphState):\n",
    "    \"\"\"\n",
    "    Determines if the incoming query is a historical question.\n",
    "    This helps in deciding whether to perform RAG or directly apply guardrails.\n",
    "    \"\"\"\n",
    "    print(\"---CLASSIFYING QUERY---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Use a simpler LLM call for classification to save tokens/latency\n",
    "    # Note: Using the same LLM for classification, but with a specific prompt.\n",
    "    classifier_prompt = ChatPromptTemplate.from_messages([\n",
    "        SystemMessage(content=\"You are a helpful assistant. Your task is to classify if a given user question is related to 'history', 'architecture', or 'engineering' of historical structures. Respond with 'YES' if it is, and 'NO' if it is not. Be strict with your classification. Examples of 'NO': current events, personal opinions, finance, medical advice, fictional scenarios.\"),\n",
    "        HumanMessage(content=f\"Is the following question historical/architectural/engineering-related? '{question}'\")\n",
    "    ])\n",
    "    classifier_chain = classifier_prompt | llm | StrOutputParser()\n",
    "\n",
    "    classification_result = classifier_chain.invoke({\"question\": question})\n",
    "    is_historical = \"YES\" in classification_result.upper()\n",
    "\n",
    "    print(f\"Query Classification: {classification_result.strip()} (Is Historical: {is_historical})\")\n",
    "    return {\"is_historical_query\": is_historical}\n",
    "\n",
    "\n",
    "# Node 2: Retrieve\n",
    "def retrieve(state: GraphState):\n",
    "    \"\"\"\n",
    "    Retrieves documents from the vector store based on the user's question.\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVING CONTEXT---\")\n",
    "    question = state[\"question\"]\n",
    "    docs = retriever.invoke(question)\n",
    "    context = [doc.page_content for doc in docs]\n",
    "    print(f\"Retrieved {len(context)} documents.\")\n",
    "    return {\"context\": context}\n",
    "\n",
    "# Node 3: Generate\n",
    "def generate(state: GraphState):\n",
    "    \"\"\"\n",
    "    Generates a response using the LLM, incorporating retrieved context if available,\n",
    "    and adhering to the system prompt with guardrails.\n",
    "    \"\"\"\n",
    "    print(\"---GENERATING RESPONSE---\")\n",
    "    question = state[\"question\"]\n",
    "    context = state[\"context\"]\n",
    "\n",
    "    # Load system prompt content from file\n",
    "    try:\n",
    "        with open(\"prompt.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "            system_prompt_content = file.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading prompt.txt in generate node: {e}\")\n",
    "        system_prompt_content = \"You are a helpful assistant.\" # Fallback\n",
    "\n",
    "    # --- FIX APPLIED HERE ---\n",
    "    # Construct the messages list directly for ChatGoogleGenerativeAI\n",
    "    # Using triple-quoted f-strings for multiline content to avoid backslash issues\n",
    "    messages = [SystemMessage(content=system_prompt_content)]\n",
    "\n",
    "    if context:\n",
    "        human_message_content = f\"\"\"Use the following retrieved context to answer the question. \\\n",
    "If the question cannot be answered from the provided context, state that you do not have sufficient information, \\\n",
    "but still adhere to your historical expert persona and guardrails.\n",
    "\n",
    "Context:\n",
    "{'   '.join(context)}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        messages.append(HumanMessage(content=human_message_content))\n",
    "    else:\n",
    "        human_message_content = f\"\"\"Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        messages.append(HumanMessage(content=human_message_content))\n",
    "\n",
    "    # Create the generation chain:\n",
    "    # Instead of `messages | llm | StrOutputParser()`,\n",
    "    # we directly invoke `llm` with the `messages` and then pipe to `StrOutputParser`.\n",
    "    rag_chain = (lambda x: llm.invoke(x[\"messages\"])) | StrOutputParser()\n",
    "\n",
    "    # Prepare input for the chain.\n",
    "    # The `messages` list is now passed as a dictionary key.\n",
    "    input_data = {\"messages\": messages}\n",
    "\n",
    "    generation_result = rag_chain.invoke(input_data)\n",
    "    print(\"Response generated.\")\n",
    "    return {\"generation\": generation_result}\n",
    "\n",
    "# 3. Define Conditional Edge\n",
    "def decide_to_retrieve(state: GraphState):\n",
    "    \"\"\"\n",
    "    Decides whether to retrieve context based on the query classification.\n",
    "    \"\"\"\n",
    "    print(\"---DECIDING TO RETRIEVE---\")\n",
    "    if state[\"is_historical_query\"]:\n",
    "        print(\"Decision: Query is historical, proceeding to retrieve.\")\n",
    "        return \"retrieve\"\n",
    "    else:\n",
    "        print(\"Decision: Query is not historical, skipping retrieval and directly generating (applying general guardrails).\")\n",
    "        return \"generate\" # Skip retrieval for non-historical questions\n",
    "\n",
    "print(\"Graph state and nodes defined.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c58a82e-33a6-4898-ae32-002daf9b97c5",
   "metadata": {},
   "source": [
    "---\n",
    "### 6. Build and Compile the LangGraph Workflow\n",
    "Now we assemble our nodes into a graph, defining the flow of execution based on the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a8ce4c2-6617-4f63-9ce5-f2508402d83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangGraph workflow compiled.\n"
     ]
    }
   ],
   "source": [
    "# --- Build the Graph ---\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"classify_query\", query_classifier)\n",
    "workflow.add_node(\"retrieve_context\", retrieve)\n",
    "workflow.add_node(\"generate_response\", generate)\n",
    "\n",
    "# Set entry point\n",
    "workflow.set_entry_point(\"classify_query\")\n",
    "\n",
    "# Add edges\n",
    "workflow.add_conditional_edges(\n",
    "    \"classify_query\",\n",
    "    decide_to_retrieve,\n",
    "    {\n",
    "        \"retrieve\": \"retrieve_context\",\n",
    "        \"generate\": \"generate_response\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# Add edge from retrieve to generate\n",
    "workflow.add_edge(\"retrieve_context\", \"generate_response\")\n",
    "\n",
    "# Set end point\n",
    "workflow.add_edge(\"generate_response\", END)\n",
    "\n",
    "# Compile the graph\n",
    "app = workflow.compile()\n",
    "\n",
    "print(\"LangGraph workflow compiled.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf018a28-db7b-4c0a-9a7e-91b370171bf2",
   "metadata": {},
   "source": [
    "---\n",
    "### 7. Run the Agent and Test Guardrails\n",
    "Let's test our RAG-enabled, stateful Historical Expert with both an on-topic and an off-topic question.\n",
    "\n",
    "#### Test Case 1: On-Topic Historical Question (RAG should activate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4c6b3d0-5f19-491a-bc6a-5a64b1cddba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test Case 1: Asking an on-topic historical question (RAG expected) ---\n",
      "User Question: Why does the Leaning Tower of Pisa lean, and what was done to fix it?\n",
      "---CLASSIFYING QUERY---\n",
      "Query Classification:  (Is Historical: False)\n",
      "---DECIDING TO RETRIEVE---\n",
      "Decision: Query is not historical, skipping retrieval and directly generating (applying general guardrails).\n",
      "{'classify_query': {'is_historical_query': False}}\n",
      "---\n",
      "---GENERATING RESPONSE---\n",
      "Response generated.\n",
      "{'generate_response': {'generation': '\\n\\n### Summary:\\nThe Leaning Tower of Pisa leans due to the unstable, soft ground beneath it, which could not support its weight evenly during construction. To stabilize the tower, engineers removed soil from under its higher side and used counterweights to reduce the tilt. These measures successfully decreased the lean by about 45 centimeters (18 inches), and the tower is now stable for future generations.\\n\\nIf you have more questions about historical structures or engineering feats, feel free to ask!\\n\\n\\\\boxed{45}'}}\n",
      "---\n",
      "---CLASSIFYING QUERY---\n",
      "Query Classification:  (Is Historical: False)\n",
      "---DECIDING TO RETRIEVE---\n",
      "Decision: Query is not historical, skipping retrieval and directly generating (applying general guardrails).\n",
      "---GENERATING RESPONSE---\n",
      "Response generated.\n",
      "\n",
      "Final Historical Expert's Response:\n",
      "\n",
      "\n",
      "### Summary:\n",
      "The Leaning Tower of Pisa leans due to the unstable ground and insufficient foundation depth during its construction in the 12th century. The tower's tilt was caused by the soft clay, sand, and shell foundation, which could not support the structure's height evenly.\n",
      "\n",
      "To stabilize the tower, engineers implemented a multi-step process in the late 20th century, notably:\n",
      "1. Removing soil from underneath the higher side of the tower to help it lean back towards a more vertical position.\n",
      "2. Using counterweights and other stabilization techniques to reduce the tilt and reinforce the foundation.\n",
      "\n",
      "These efforts have made the tower stable enough to withstand natural forces like earthquakes. The tower still leans, but it is now safe and its tilt has been significantly reduced.\n",
      "\n",
      "Would you like more details on any specific part of this process?\n",
      "\n",
      "### Final Answer:\n",
      "The Leaning Tower of Pisa leans because it was built on unstable ground with a shallow foundation, causing one side to sink more than the other. To fix this, engineers removed soil from under the higher side and employed stabilization techniques to reduce the tilt. These measures have made the tower stable, though it retains its iconic lean.\n",
      "\n",
      "\\boxed{\\text{The Leaning Tower of Pisa leans due to unstable ground and was stabilized by removing soil from under the higher side and other reinforcement techniques.}}\n"
     ]
    }
   ],
   "source": [
    "# Test Case 1: On-Topic Historical Question (RAG should activate)\n",
    "\n",
    "print(\"\\n--- Test Case 1: Asking an on-topic historical question (RAG expected) ---\")\n",
    "historical_question = \"Why does the Leaning Tower of Pisa lean, and what was done to fix it?\"\n",
    "print(f\"User Question: {historical_question}\")\n",
    "\n",
    "try:\n",
    "    inputs = {\"question\": historical_question, \"context\": [], \"generation\": \"\", \"is_historical_query\": False}\n",
    "    for s in app.stream(inputs):\n",
    "        print(s)\n",
    "        print(\"---\")\n",
    "    final_state = app.invoke(inputs)\n",
    "    print(\"\\nFinal Historical Expert's Response:\")\n",
    "    print(final_state[\"generation\"])\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during the historical question API call: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027c73a9-885a-49c0-8d39-0735f74ad8ee",
   "metadata": {},
   "source": [
    "---\n",
    "### Test Case 2: Off-Topic Question (Guardrails should activate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0056af7c-afbe-4ef9-ba6a-07c156a13aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test Case 2: Asking an off-topic question (Guardrails expected) ---\n",
      "User Question: Can you give me a detailed analysis of the current stock market trends for tech companies?\n",
      "---CLASSIFYING QUERY---\n",
      "Query Classification: **Summary and Answer:**\n",
      "The question is about current stock market trends for tech companies, which falls under current events and finance. It does not relate to history, architecture, or engineering of historical structures.\n",
      "\n",
      "Therefore, the classification is:\n",
      "\n",
      "\\boxed{NO} (Is Historical: False)\n",
      "---DECIDING TO RETRIEVE---\n",
      "Decision: Query is not historical, skipping retrieval and directly generating (applying general guardrails).\n",
      "{'classify_query': {'is_historical_query': False}}\n",
      "---\n",
      "---GENERATING RESPONSE---\n",
      "Response generated.\n",
      "{'generate_response': {'generation': '\\n\\n### Answer:\\nThank you for your question. However, as a Historical Expert, my area of expertise is focused on historical events, architectural marvels, and engineering feats. Unfortunately, I cannot provide a detailed analysis of current stock market trends for tech companies, as this falls outside my domain.\\n\\nIf you have any questions related to history, architecture, or engineering, I’d be more than happy to assist you.\\n\\n### Summary:\\nThe question about current stock market trends for tech companies is outside my role as a Historical Expert. I acknowledged the question, explained my limitations, and redirected the user to my area of expertise. This response adheres to my constraints and maintains a professional tone.\\n\\n\\\\boxed{\\\\text{As a Historical Expert, I cannot provide analysis on current stock market trends. Please ask about history, architecture, or engineering for assistance.}}'}}\n",
      "---\n",
      "---CLASSIFYING QUERY---\n",
      "Query Classification: The question is not related to the history, architecture, or engineering of historical structures, as it pertains to current financial trends. Thus, the classification is:\n",
      "\n",
      "\\boxed{NO} (Is Historical: False)\n",
      "---DECIDING TO RETRIEVE---\n",
      "Decision: Query is not historical, skipping retrieval and directly generating (applying general guardrails).\n",
      "---GENERATING RESPONSE---\n",
      "Response generated.\n",
      "\n",
      "Final Historical Expert's Response:\n",
      "\n",
      "\n",
      "### Summary:\n",
      "The user's question about current stock market trends for tech companies is outside my scope of expertise, which is focused on historical events, architecture, and engineering. I decline to answer while offering alternative topics within my domain.\n",
      "\n",
      "### Final Answer:\n",
      "Thank you for your question. As a historical expert, my focus is on historical events, architectural marvels, and engineering feats, so I’m unable to provide insights into current stock market trends. However, I’d be delighted to discuss historical aspects of technology, such as the evolution of computing, the history of Silicon Valley, or notable engineering achievements in the tech industry. Would you like to explore any of these topics instead? I’m here to assist with questions within my area of expertise.\n",
      "\n",
      "For current stock market trends, I recommend consulting a financial expert or a dedicated financial news source.\n",
      "\n",
      "Is there another topic I can help you with?\n",
      "\n",
      "\\boxed{\\text{Unable to assist with current stock market trends. Focused on historical and engineering topics.}}\n"
     ]
    }
   ],
   "source": [
    "# Test Case 2: Off-Topic Question (Guardrails should activate)\n",
    "print(\"\\n--- Test Case 2: Asking an off-topic question (Guardrails expected) ---\")\n",
    "off_topic_question = \"Can you give me a detailed analysis of the current stock market trends for tech companies?\"\n",
    "print(f\"User Question: {off_topic_question}\")\n",
    "\n",
    "try:\n",
    "    inputs = {\"question\": off_topic_question, \"context\": [], \"generation\": \"\", \"is_historical_query\": False}\n",
    "    for s in app.stream(inputs):\n",
    "        print(s)\n",
    "        print(\"---\")\n",
    "    final_state = app.invoke(inputs)\n",
    "    print(\"\\nFinal Historical Expert's Response:\")\n",
    "    print(final_state[\"generation\"])\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during the off-topic question API call: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b189e587-bf86-4031-b345-62e20b86fd44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef8597f-4701-4306-bce3-3cb3e674b3be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
