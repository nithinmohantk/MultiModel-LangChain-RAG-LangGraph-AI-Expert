{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89c716c1-6924-4820-8574-74b0e42fa963",
   "metadata": {},
   "source": [
    "## LangChain RAG Expert with LangGraph State Management (using HuggingFace)\n",
    "This Jupyter Notebook demonstrates how to build a sophisticated AI agent using LangChain and LangGraph, now integrated with HuggingFace Platform for the Large Language Model. The agent will act as a \"Helpful Historical Expert\" with the following key enhancements:\n",
    "\n",
    "- **Retrieval-Augmented Generation (RAG):** The agent will query a local knowledge base (a text file) to retrieve relevant information before generating a response, ensuring factual accuracy and reducing hallucinations.\n",
    "- **LangGraph for State Management:** We will use LangGraph to define a stateful workflow, allowing the agent to manage its internal state (e.g., current question, retrieved context) and execute steps like retrieval and response generation conditionally.\n",
    "- **Enhanced Guardrails:** The detailed system prompt from prompt.txt will continue to guide the AI's persona, tone, and adherence to safety and scope constraints.\n",
    "- **Ollama Integration:** The core language model for generation will be a local Ollama model (e.g., llama2).\n",
    "\n",
    "We will test the system with both an on-topic historical question (which should leverage RAG) and an off-topic question (which should trigger the guardrails).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1286f8-dd54-4984-a37f-ec12db84b082",
   "metadata": {},
   "source": [
    "### 1. Setup and Installation\n",
    "First, we need to install all the necessary libraries. This includes LangChain components, langchain-google-genai for Gemini integration, langchain-openai for embeddings (as Gemini's native embeddings might require re-indexing the vector store, keeping OpenAI for consistency here), LangGraph for state management, FAISS for vector storage, and Tiktoken for tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7d0e8ea-0a5b-4568-bb89-fa0e05d1c4cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in links: /usr/share/pip-wheels\n",
      "Requirement already satisfied: langchain in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (0.3.26)\n",
      "Requirement already satisfied: langchain-community in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (0.3.27)\n",
      "Requirement already satisfied: langchain-huggingface in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (0.3.0)\n",
      "Requirement already satisfied: langchain-openai in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (0.3.28)\n",
      "Requirement already satisfied: langgraph in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (0.5.3)\n",
      "Requirement already satisfied: faiss-cpu in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (1.11.0.post1)\n",
      "Requirement already satisfied: tiktoken in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (0.9.0)\n",
      "Requirement already satisfied: huggingface_hub in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (0.33.4)\n",
      "Requirement already satisfied: transformers in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (4.53.2)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (from langchain) (0.3.68)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (from langchain) (0.4.5)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (from langchain) (2.11.7)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from langchain) (2.0.29)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from langchain-community) (3.9.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from langchain-community) (8.2.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from langchain-community) (0.6.4)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (from langchain-community) (2.10.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (from langchain-community) (0.4.1)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from langchain-community) (1.26.4)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (from langchain-huggingface) (0.21.2)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.86.0 in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (from langchain-openai) (1.95.1)\n",
      "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.1.0 in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (from langgraph) (2.1.0)\n",
      "Requirement already satisfied: langgraph-prebuilt<0.6.0,>=0.5.0 in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (from langgraph) (0.5.2)\n",
      "Requirement already satisfied: langgraph-sdk<0.2.0,>=0.1.42 in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (from langgraph) (0.1.72)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (from langgraph) (3.5.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from faiss-cpu) (23.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from tiktoken) (2023.10.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from huggingface_hub) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from huggingface_hub) (2023.10.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from huggingface_hub) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (from huggingface_hub) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (from huggingface_hub) (1.1.5)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.21.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (1.33)\n",
      "Requirement already satisfied: ormsgpack>=1.10.0 in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph) (1.10.0)\n",
      "Requirement already satisfied: httpx>=0.25.2 in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.10.1 in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/b7177933-117a-47b6-b9dd-b47a167f2420/.local/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.86.0->langchain-openai) (1.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (2.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/envs/anaconda-ai-2024.04-py310/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in links: /usr/share/pip-wheels\n",
      "Collecting protobuf==3.20.3\n",
      "  Downloading protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (679 bytes)\n",
      "Downloading protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[38;5;70m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: protobuf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.3\n",
      "    Uninstalling protobuf-3.20.3:\n",
      "      Successfully uninstalled protobuf-3.20.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.11.0 requires libclang>=13.0.0, which is not installed.\n",
      "tensorflow 2.11.0 requires tensorflow-io-gcs-filesystem>=0.23.1; platform_machine != \"arm64\" or platform_system != \"Darwin\", which is not installed.\n",
      "grpcio-status 1.62.3 requires protobuf>=4.21.6, but you have protobuf 3.20.3 which is incompatible.\n",
      "tensorflow 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed protobuf-3.20.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install necessary libraries\n",
    "%pip install -U langchain langchain-community langchain-huggingface langchain-openai langgraph faiss-cpu tiktoken huggingface_hub transformers\n",
    "%pip install protobuf==3.20.3 --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca40ccb2-c443-47ec-8ee1-68f50955fd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_TOKEN\"] = \"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\"\n",
    "os.environ[\"OPENAI_API_KEY\"]  = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f06490e8-378f-4bbe-82e3-3c31de856b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-16 08:41:20.053420: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_TOKEN is set.\n",
      "OPENAI_API_KEY is set.\n",
      "Python executable: /opt/conda/envs/anaconda-ai-2024.04-py310/bin/python\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain, LangGraph, HuggingFace, and OpenAI setup complete.\n",
      "\n",
      "If you still encounter 'ModuleNotFoundError' after running this cell, please try:\n",
      "1. Restarting your Jupyter kernel (Kernel -> Restart Kernel...)\n",
      "2. Running this setup cell again.\n",
      "3. Ensure you have pulled the required HuggingFace model or it's accessible via your HF_TOKEN.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from openai import OpenAI\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline # Changed to ChatHuggingFace for HuggingFace\n",
    "from langchain_openai import OpenAIEmbeddings # Still using OpenAI for embeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated, List\n",
    "import operator\n",
    "# Removed specific transformers imports as we will use HuggingFaceHub directly\n",
    "from langchain_community.llms import HuggingFaceHub # Import HuggingFaceHub for direct LLM access\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "# --- Set your HuggingFace Hub API Token ---\n",
    "# It's highly recommended to set this as an environment variable for security.\n",
    "# You can do this in your terminal before starting Jupyter:\n",
    "# export HF_TOKEN='your_huggingface_token_here' (Linux/macOS)\n",
    "# $env:HF_TOKEN='your_huggingface_token_here' (PowerShell)\n",
    "#\n",
    "# You will also need your OpenAI API key for embeddings:\n",
    "# export OPENAI_API_KEY='your_openai_api_key_here'\n",
    "#\n",
    "# If you must set them directly in the notebook (NOT recommended for production):\n",
    "# os.environ[\"HF_TOKEN\"] = \"YOUR_ACTUAL_HUGGINGFACE_TOKEN\"\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_ACTUAL_OPENAI_API_KEY\"\n",
    "os.environ[\"HF_TOKEN\"] = \"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\"\n",
    "os.environ[\"OPENAI_API_KEY\"]  = \"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\"\n",
    "\n",
    "\n",
    "# Verify API keys/tokens are set\n",
    "if \"HF_TOKEN\" not in os.environ:\n",
    "    print(\"WARNING: HF_TOKEN environment variable not set.\")\n",
    "    print(\"Please set it before proceeding, or uncomment the line above to set it directly (not recommended).\")\n",
    "else:\n",
    "    print(\"HF_TOKEN is set.\")\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    print(\"WARNING: OPENAI_API_KEY environment variable not set (needed for embeddings).\")\n",
    "    print(\"Please set it before proceeding, or uncomment the line above to set it directly (not recommended).\")\n",
    "else:\n",
    "    print(\"OPENAI_API_KEY is set.\")\n",
    "\n",
    "\n",
    "# Print the Python executable path to help debug environment issues\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "\n",
    "# Initialize the HuggingFace model for generation\n",
    "# --- IMPORTANT: MODEL ACCESS AND TOKEN PERMISSIONS ---\n",
    "# If you encounter a \"403 Forbidden\" or \"GatedRepoError\", it means your HuggingFace token\n",
    "# does not have the necessary permissions or you haven't accepted the model's terms of use.\n",
    "#\n",
    "# **ACTION REQUIRED TO RESOLVE 403 FORBIDDEN / GATEDREPOERROR:**\n",
    "# 1.  **Visit your HuggingFace token settings:** Go to **https://huggingface.co/settings/tokens**\n",
    "# 2.  **Log in** to your HuggingFace account.\n",
    "# 3.  **Review and Edit your Token:**\n",
    "#     * Click on the token you are using (or create a new one).\n",
    "#     * Under \"Repository access\" or \"Fine-grained permissions\", ensure that **\"Public gated repositories\"**\n",
    "#         access is **enabled**. This is crucial for models that require accepting terms.\n",
    "#     * For broader access to all types of repositories (public, private, and gated), you can select \"All repositories\".\n",
    "# 4.  **Accept Model Terms (if applicable):** If you are trying to use a specific model (e.g., `google/gemma-3-1b-pt` or `mistralai/Mistral-7B-Instruct-v0.3`), visit its model page on HuggingFace Hub (e.g., https://huggingface.co/google/gemma-3-1b-pt or https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3). Look for a section or button to **\"Request access\" or \"Accept license\"** and follow any instructions.\n",
    "# 5.  **Restart Jupyter Kernel:** After updating your token's permissions or accepting terms, **RESTART YOUR JUPYTER KERNEL** (Kernel -> Restart Kernel...) to ensure the new permissions are loaded.\n",
    "\n",
    "# We will use \"HuggingFaceH4/zephyr-7b-beta\" as a default, as it's a commonly used and generally accessible instruct model.\n",
    "# If you have requested and been granted access to other models (and updated your token permissions), you can try them.\n",
    "# HF_MODEL_ID = \"HuggingFaceH4/zephyr-7b-beta\" # Publicly available instruct model\n",
    "# Example of a gated model (uncomment if you have access and permissions):\n",
    "# HF_MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "# HF_MODEL_ID = \"google/gemma-3-1b-pt\"\n",
    "HF_MODEL_ID = \"google/gemma-3-1b-it\" # Publicly available instruct model\n",
    "\n",
    "HUGGINGFACE_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "if not HUGGINGFACE_TOKEN:\n",
    "    raise ValueError(\"HF_TOKEN environment variable not set.\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(HF_MODEL_ID, token=HUGGINGFACE_TOKEN)\n",
    "model = AutoModelForCausalLM.from_pretrained(HF_MODEL_ID, token=HUGGINGFACE_TOKEN)\n",
    "\n",
    "# Create a text generation pipeline\n",
    "hf_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer, # Explicitly pass the tokenizer\n",
    "    max_new_tokens=500,\n",
    "    temperature=0.7,\n",
    "    do_sample=True,\n",
    "    token=HUGGINGFACE_TOKEN,\n",
    "    device=0 # Uncomment if you have a GPU and want to use it\n",
    ")\n",
    "\n",
    "# Wrap the HuggingFace pipeline with ChatHuggingFace\n",
    "hf_llm_pipeline = HuggingFacePipeline(pipeline=hf_pipeline)\n",
    "llm = ChatHuggingFace(llm=hf_llm_pipeline)\n",
    "\n",
    "# --- FIX APPLIED HERE: Explicitly set the tokenizer on the llm object ---\n",
    "# This addresses the 'NoneType' object has no attribute 'apply_chat_template' error\n",
    "# by ensuring the tokenizer is available for chat templating within ChatHuggingFace.\n",
    "if not hasattr(llm, 'tokenizer') or llm.tokenizer is None:\n",
    "    llm.tokenizer = tokenizer\n",
    "    print(\"Explicitly set tokenizer on ChatHuggingFace instance to resolve apply_chat_template error.\")\n",
    "\n",
    "\n",
    "# Initialize OpenAIEmbeddings for RAG (keeping consistent with previous notebooks)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "print(\"LangChain, LangGraph, HuggingFace, and OpenAI setup complete.\")\n",
    "print(\"\\nIf you still encounter 'ModuleNotFoundError' after running this cell, please try:\")\n",
    "print(\"1. Restarting your Jupyter kernel (Kernel -> Restart Kernel...)\")\n",
    "print(\"2. Running this setup cell again.\")\n",
    "print(\"3. Ensure you have pulled the required HuggingFace model or it's accessible via your HF_TOKEN.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aabcec8-c838-4f1a-b258-31c520a8c29d",
   "metadata": {},
   "source": [
    "### 2. Create the prompt.txt File\n",
    "Create or update a file named prompt.txt in the same directory as this Jupyter Notebook. This file will contain the detailed system prompt with all the guardrails for your Historical Expert.\n",
    "\n",
    "prompt.txt content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28e45fe-3527-4e8f-a9bc-0072be8af0e0",
   "metadata": {},
   "source": [
    "### 3. Create Knowledge Base File (pisa_history.txt)\n",
    "Create a new file named pisa_history.txt in the same directory as this Jupyter Notebook. This file will serve as our knowledge base for RAG.\n",
    "\n",
    "pisa_history.txt content (example, feel free to expand):\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7e23c0-1fa4-4749-8379-0c0d7c72069a",
   "metadata": {},
   "source": [
    "### 4. RAG Setup: Create Retriever\n",
    "Here, we'll load our pisa_history.txt file, split it into manageable chunks, create embeddings for these chunks, and then store them in a FAISS vector store to enable efficient retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf1251a3-beb2-48fc-8332-4f97cd50f159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded RAG document from 'pisa_history.txt'\n",
      "Split document into 4 chunks.\n",
      "FAISS vector store created.\n",
      "Retriever created.\n"
     ]
    }
   ],
   "source": [
    "# --- RAG Setup ---\n",
    "rag_file_path = \"pisa_history.txt\"\n",
    "\n",
    "# 1. Load the document\n",
    "try:\n",
    "    loader = TextLoader(rag_file_path, encoding=\"utf-8\")\n",
    "    documents = loader.load()\n",
    "    print(f\"Successfully loaded RAG document from '{rag_file_path}'\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The RAG file '{rag_file_path}' was not found. Please create it.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the RAG document: {e}\")\n",
    "    exit()\n",
    "\n",
    "# 2. Split the document into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(documents)\n",
    "print(f\"Split document into {len(splits)} chunks.\")\n",
    "\n",
    "# 3. Create a FAISS vector store from the chunks and embeddings\n",
    "vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings)\n",
    "print(\"FAISS vector store created.\")\n",
    "\n",
    "# 4. Create a retriever\n",
    "retriever = vectorstore.as_retriever()\n",
    "print(\"Retriever created.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525a4ce6-4a15-41e8-8981-a3e59cc1d93b",
   "metadata": {},
   "source": [
    "---\n",
    "### 5. LangGraph Setup: Define Graph State and Nodes\n",
    "We will define the state of our graph and the individual nodes (functions) that represent the steps in our agent's workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3c91ab4-6376-48d9-a10b-6204077ed923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph state and nodes defined.\n"
     ]
    }
   ],
   "source": [
    "# --- LangGraph Setup ---\n",
    "\n",
    "# 1. Define Graph State\n",
    "# This defines the object that is passed between nodes in the graph.\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: The user's question.\n",
    "        context: Retrieved context from the RAG system.\n",
    "        generation: The final generated answer from the LLM.\n",
    "        is_historical_query: A flag to determine if the query is historical.\n",
    "    \"\"\"\n",
    "    question: str\n",
    "    context: Annotated[List[str], operator.add] # Context will be accumulated\n",
    "    generation: str\n",
    "    is_historical_query: bool # New field to control flow\n",
    "\n",
    "# 2. Define Nodes (Functions)\n",
    "\n",
    "# Node 1: Query Classifier\n",
    "# This node determines if the incoming query is within the historical expert's scope.\n",
    "def query_classifier(state: GraphState):\n",
    "    \"\"\"\n",
    "    Determines if the incoming query is a historical question.\n",
    "    This helps in deciding whether to perform RAG or directly apply guardrails.\n",
    "    \"\"\"\n",
    "    print(\"---CLASSIFYING QUERY---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Use a simpler LLM call for classification to save tokens/latency\n",
    "    # Note: Using the same LLM for classification, but with a specific prompt.\n",
    "    classifier_prompt = ChatPromptTemplate.from_messages([\n",
    "        SystemMessage(content=\"You are a helpful assistant. Your task is to classify if a given user question is related to 'history', 'architecture', or 'engineering' of historical structures. Respond with 'YES' if it is, and 'NO' if it is not. Be strict with your classification. Examples of 'NO': current events, personal opinions, finance, medical advice, fictional scenarios.\"),\n",
    "        HumanMessage(content=f\"Is the following question historical/architectural/engineering-related? '{question}'\")\n",
    "    ])\n",
    "    classifier_chain = classifier_prompt | llm | StrOutputParser()\n",
    "\n",
    "    classification_result = classifier_chain.invoke({\"question\": question})\n",
    "    is_historical = \"YES\" in classification_result.upper()\n",
    "\n",
    "    print(f\"Query Classification: {classification_result.strip()} (Is Historical: {is_historical})\")\n",
    "    return {\"is_historical_query\": is_historical}\n",
    "\n",
    "\n",
    "# Node 2: Retrieve\n",
    "def retrieve(state: GraphState):\n",
    "    \"\"\"\n",
    "    Retrieves documents from the vector store based on the user's question.\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVING CONTEXT---\")\n",
    "    question = state[\"question\"]\n",
    "    docs = retriever.invoke(question)\n",
    "    context = [doc.page_content for doc in docs]\n",
    "    print(f\"Retrieved {len(context)} documents.\")\n",
    "    return {\"context\": context}\n",
    "\n",
    "# Node 3: Generate\n",
    "def generate(state: GraphState):\n",
    "    \"\"\"\n",
    "    Generates a response using the LLM, incorporating retrieved context if available,\n",
    "    and adhering to the system prompt with guardrails.\n",
    "    \"\"\"\n",
    "    print(\"---GENERATING RESPONSE---\")\n",
    "    question = state[\"question\"]\n",
    "    context = state[\"context\"]\n",
    "\n",
    "    # Load system prompt content from file\n",
    "    try:\n",
    "        with open(\"prompt.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "            system_prompt_content = file.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading prompt.txt in generate node: {e}\")\n",
    "        system_prompt_content = \"You are a helpful assistant.\" # Fallback\n",
    "\n",
    "    # Construct the messages list directly for ChatOllama\n",
    "    # Using triple-quoted f-strings for multiline content\n",
    "    messages = [SystemMessage(content=system_prompt_content)]\n",
    "\n",
    "    if context:\n",
    "        human_message_content = f\"\"\"Use the following retrieved context to answer the question. \\\n",
    "If the question cannot be answered from the provided context, state that you do not have sufficient information, \\\n",
    "but still adhere to your historical expert persona and guardrails.\n",
    "\n",
    "Context:\n",
    "{'  '.join(context)}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        messages.append(HumanMessage(content=human_message_content))\n",
    "    else:\n",
    "        human_message_content = f\"\"\"Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        messages.append(HumanMessage(content=human_message_content))\n",
    "\n",
    "    # Create the generation chain:\n",
    "    # Directly invoke `llm` with the `messages` and then pipe to `StrOutputParser`.\n",
    "    rag_chain = (lambda x: llm.invoke(x[\"messages\"])) | StrOutputParser()\n",
    "\n",
    "    # Prepare input for the chain.\n",
    "    input_data = {\"messages\": messages}\n",
    "\n",
    "    generation_result = rag_chain.invoke(input_data)\n",
    "    print(\"Response generated.\")\n",
    "    return {\"generation\": generation_result}\n",
    "\n",
    "# 3. Define Conditional Edge\n",
    "def decide_to_retrieve(state: GraphState):\n",
    "    \"\"\"\n",
    "    Decides whether to retrieve context based on the query classification.\n",
    "    \"\"\"\n",
    "    print(\"---DECIDING TO RETRIEVE---\")\n",
    "    if state[\"is_historical_query\"]:\n",
    "        print(\"Decision: Query is historical, proceeding to retrieve.\")\n",
    "        return \"retrieve\"\n",
    "    else:\n",
    "        print(\"Decision: Query is not historical, skipping retrieval and directly generating (applying general guardrails).\")\n",
    "        return \"generate\" # Skip retrieval for non-historical questions\n",
    "\n",
    "print(\"Graph state and nodes defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c58a82e-33a6-4898-ae32-002daf9b97c5",
   "metadata": {},
   "source": [
    "---\n",
    "### 6. Build and Compile the LangGraph Workflow\n",
    "Now we assemble our nodes into a graph, defining the flow of execution based on the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a8ce4c2-6617-4f63-9ce5-f2508402d83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangGraph workflow compiled.\n"
     ]
    }
   ],
   "source": [
    "# --- Build the Graph ---\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"classify_query\", query_classifier)\n",
    "workflow.add_node(\"retrieve_context\", retrieve)\n",
    "workflow.add_node(\"generate_response\", generate)\n",
    "\n",
    "# Set entry point\n",
    "workflow.set_entry_point(\"classify_query\")\n",
    "\n",
    "# Add edges\n",
    "workflow.add_conditional_edges(\n",
    "    \"classify_query\",\n",
    "    decide_to_retrieve,\n",
    "    {\n",
    "        \"retrieve\": \"retrieve_context\",\n",
    "        \"generate\": \"generate_response\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# Add edge from retrieve to generate\n",
    "workflow.add_edge(\"retrieve_context\", \"generate_response\")\n",
    "\n",
    "# Set end point\n",
    "workflow.add_edge(\"generate_response\", END)\n",
    "\n",
    "# Compile the graph\n",
    "app = workflow.compile()\n",
    "\n",
    "print(\"LangGraph workflow compiled.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf018a28-db7b-4c0a-9a7e-91b370171bf2",
   "metadata": {},
   "source": [
    "---\n",
    "### 7. Run the Agent and Test Guardrails\n",
    "Let's test our RAG-enabled, stateful Historical Expert with both an on-topic and an off-topic question.\n",
    "\n",
    "#### Test Case 1: On-Topic Historical Question (RAG should activate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4c6b3d0-5f19-491a-bc6a-5a64b1cddba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test Case 1: Asking an on-topic historical question (RAG expected) ---\n",
      "User Question: Why does the Leaning Tower of Pisa lean, and what was done to fix it?\n",
      "---CLASSIFYING QUERY---\n",
      "Query Classification: <bos><start_of_turn>user\n",
      "You are a helpful assistant. Your task is to classify if a given user question is related to 'history', 'architecture', or 'engineering' of historical structures. Respond with 'YES' if it is, and 'NO' if it is not. Be strict with your classification. Examples of 'NO': current events, personal opinions, finance, medical advice, fictional scenarios.\n",
      "\n",
      "Is the following question historical/architectural/engineering-related? 'Why does the Leaning Tower of Pisa lean, and what was done to fix it?'<end_of_turn>\n",
      "<start_of_turn>model\n",
      "YES (Is Historical: True)\n",
      "---DECIDING TO RETRIEVE---\n",
      "Decision: Query is historical, proceeding to retrieve.\n",
      "{'classify_query': {'is_historical_query': True}}\n",
      "---\n",
      "---RETRIEVING CONTEXT---\n",
      "Retrieved 4 documents.\n",
      "{'retrieve_context': {'context': ['Over the centuries, various efforts were made to correct or prevent the collapse of the tower. In 1838, architect Alessandro Gherardesca dug a pathway around the base to make the base visible, which caused the tower to lean even more. Benito Mussolini ordered that the tower be returned to a vertical position, and concrete was poured into the foundations in 1934, which also worsened the lean.\\n\\nThe most significant stabilization efforts took place from 1990 to 2001. An international committee of experts, led by Michele Jamiolkowski, undertook a major project. They used counterweights (600 tonnes of lead ingots) on the north side of the base and, more effectively, soil extraction. Soil was carefully removed from underneath the north side of the foundations, causing the tower to slowly settle back towards the north, reducing its tilt by about 45 cm (17.7 inches). This brought the lean back to what it was in 1838. The tower was reopened to the public in December 2001.', \"The Leaning Tower of Pisa is the campanile, or freestanding bell tower, of the cathedral of Pisa, Italy. It is situated behind the Pisa Cathedral and is the third oldest structure in Pisa's Cathedral Square (Piazza del Duomo), after the cathedral and the Pisa Baptistry.\\n\\nThe tower is known for its unintended tilt to one side, which began during construction in the 12th century due to soft ground that could not properly support the structure's weight. Construction began in August 1173. The tower was designed to be perfectly vertical, but it began to lean by the time the third floor was built in 1178. The soft soil, composed of clay, fine sand, and shells, was unstable. This initial lean was towards the north.\", 'Today, the Leaning Tower of Pisa is considered stable for at least another 200 years. Its unique tilt continues to attract millions of tourists worldwide.', 'Construction was halted for almost a century, which allowed the underlying soil to settle and compact, preventing the tower from toppling. When construction resumed in 1272, under Giovanni di Simone, the engineers tried to compensate for the tilt by building the upper floors with one side taller than the other. This caused the tower to start leaning in the opposite direction (south), creating a slight curve. Construction was again halted in 1284.\\n\\nThe seventh floor was completed in 1319, and the bell-chamber was finally added in 1372 by Tommaso di Andrea Pisano, who incorporated Gothic elements. The tower has 294 steps on the north side and 296 steps on the south side, as some steps had to be removed due to the lean.']}}\n",
      "---\n",
      "---GENERATING RESPONSE---\n",
      "Response generated.\n",
      "{'generate_response': {'generation': '<bos><start_of_turn>user\\n# SYSTEM ROLE DEFINITION\\nYou are a highly knowledgeable and ethical Historical Expert. Your primary function is to provide accurate, detailed, and contextually rich information regarding historical events, architectural marvels, and engineering feats.\\n\\n## Expertise:\\n- Deep understanding of world history, particularly ancient and medieval periods.\\n- Specialized knowledge in architecture, construction techniques, and engineering challenges of historical structures.\\n- Ability to synthesize complex historical data into clear, accessible explanations.\\n\\n## Personality & Tone:\\n- **Helpful and Informative:** Always aim to assist the user in understanding history.\\n- **Objective and Neutral:** Present facts without personal bias or speculation.\\n- **Respectful:** Maintain a professional and respectful tone, even when discussing sensitive historical topics.\\n- **Concise but Comprehensive:** Provide enough detail to be informative without being overly verbose.\\n\\n## CONSTRAINTS & GUARDRAILS:\\n\\n1.  **Scope Adherence:** Only answer questions directly related to history, architecture, engineering, or related cultural contexts. If a question falls outside this scope (e.g., current events, personal opinions, future predictions, medical advice, legal advice, fictional scenarios), politely decline to answer and redirect the user back to your area of expertise.\\n2.  **Factuality:** All information provided MUST be historically accurate and verifiable. If you are unsure or the information is ambiguous/disputed by historians, state this clearly. Do not hallucinate facts or dates.\\n3.  **No Harmful Content:** Absolutely do not generate content that is hateful, discriminatory, violent, sexually explicit, or promotes illegal activities. If a user attempts to solicit such content, refuse directly and explain that you cannot assist with harmful requests.\\n4.  **No Personal Information:** Do not ask for or reveal any Personally Identifiable Information (PII) of yourself or others.\\n5.  **Ethical Considerations:** When discussing potentially sensitive historical events (e.g., wars, conflicts, societal inequalities), do so with an emphasis on historical facts and their impact, avoiding glorification of violence or prejudice.\\n6.  **\"I don\\'t know\" Policy:** If you genuinely do not have the information or cannot confidently answer a question without speculating, state \"As a historical expert, I do not have sufficient information to provide an accurate answer to that specific query.\" Do not invent an answer.\\n7.  **Language:** Respond only in English.\\n\\n## RESPONSE FORMAT:\\n\\n-   Start your response by acknowledging the user\\'s question.\\n-   Provide the answer in clear, well-structured paragraphs.\\n-   Use bullet points or numbered lists for complex information or multiple points where appropriate.\\n-   Conclude your answer by offering further assistance within your domain.\\n\\n\\nUse the following retrieved context to answer the question. If the question cannot be answered from the provided context, state that you do not have sufficient information, but still adhere to your historical expert persona and guardrails.\\n\\nContext:\\nOver the centuries, various efforts were made to correct or prevent the collapse of the tower. In 1838, architect Alessandro Gherardesca dug a pathway around the base to make the base visible, which caused the tower to lean even more. Benito Mussolini ordered that the tower be returned to a vertical position, and concrete was poured into the foundations in 1934, which also worsened the lean.\\n\\nThe most significant stabilization efforts took place from 1990 to 2001. An international committee of experts, led by Michele Jamiolkowski, undertook a major project. They used counterweights (600 tonnes of lead ingots) on the north side of the base and, more effectively, soil extraction. Soil was carefully removed from underneath the north side of the foundations, causing the tower to slowly settle back towards the north, reducing its tilt by about 45 cm (17.7 inches). This brought the lean back to what it was in 1838. The tower was reopened to the public in December 2001.  The Leaning Tower of Pisa is the campanile, or freestanding bell tower, of the cathedral of Pisa, Italy. It is situated behind the Pisa Cathedral and is the third oldest structure in Pisa\\'s Cathedral Square (Piazza del Duomo), after the cathedral and the Pisa Baptistry.\\n\\nThe tower is known for its unintended tilt to one side, which began during construction in the 12th century due to soft ground that could not properly support the structure\\'s weight. Construction began in August 1173. The tower was designed to be perfectly vertical, but it began to lean by the time the third floor was built in 1178. The soft soil, composed of clay, fine sand, and shells, was unstable. This initial lean was towards the north.  Today, the Leaning Tower of Pisa is considered stable for at least another 200 years. Its unique tilt continues to attract millions of tourists worldwide.  Construction was halted for almost a century, which allowed the underlying soil to settle and compact, preventing the tower from toppling. When construction resumed in 1272, under Giovanni di Simone, the engineers tried to compensate for the tilt by building the upper floors with one side taller than the other. This caused the tower to start leaning in the opposite direction (south), creating a slight curve. Construction was again halted in 1284.\\n\\nThe seventh floor was completed in 1319, and the bell-chamber was finally added in 1372 by Tommaso di Andrea Pisano, who incorporated Gothic elements. The tower has 294 steps on the north side and 296 steps on the south side, as some steps had to be removed due to the lean.\\n\\nQuestion: Why does the Leaning Tower of Pisa lean, and what was done to fix it?\\n\\nAnswer:<end_of_turn>\\n<start_of_turn>model\\nThe Leaning Tower of Pisa leans due to the unstable soil beneath its foundations. Construction began in August 1173, and the tower was designed to be perfectly vertical, but soft soil caused it to lean by the time the third floor was built in 1178. The tower was built with a design that was not suited to the local soil conditions, which allowed the ground to settle unevenly and create a slope. In 1990 to 2001, an international committee of experts used counterweights (600 tonnes of lead ingots) on the north side of the base and soil extraction to slowly settle the tower back towards its original position. This involved removing soil from underneath the north side of the foundations, reducing the tilt by about 45 cm. The tower was reopened to the public in December 2001.\\n\\nFurther information:\\n\\n*   Construction began in August 1173.\\n*   The tower was designed to be perfectly vertical, but it began to lean by the time the third floor was built in 1178.\\n*   The seventh floor was completed in 1319, and the bell-chamber was added in 1372.\\n*   The tower has 294 steps on the north side and 296 steps on the south side.\\n'}}\n",
      "---\n",
      "---CLASSIFYING QUERY---\n",
      "Query Classification: <bos><start_of_turn>user\n",
      "You are a helpful assistant. Your task is to classify if a given user question is related to 'history', 'architecture', or 'engineering' of historical structures. Respond with 'YES' if it is, and 'NO' if it is not. Be strict with your classification. Examples of 'NO': current events, personal opinions, finance, medical advice, fictional scenarios.\n",
      "\n",
      "Is the following question historical/architectural/engineering-related? 'Why does the Leaning Tower of Pisa lean, and what was done to fix it?'<end_of_turn>\n",
      "<start_of_turn>model\n",
      "YES (Is Historical: True)\n",
      "---DECIDING TO RETRIEVE---\n",
      "Decision: Query is historical, proceeding to retrieve.\n",
      "---RETRIEVING CONTEXT---\n",
      "Retrieved 4 documents.\n",
      "---GENERATING RESPONSE---\n",
      "Response generated.\n",
      "\n",
      "Final Historical Expert's Response:\n",
      "<bos><start_of_turn>user\n",
      "# SYSTEM ROLE DEFINITION\n",
      "You are a highly knowledgeable and ethical Historical Expert. Your primary function is to provide accurate, detailed, and contextually rich information regarding historical events, architectural marvels, and engineering feats.\n",
      "\n",
      "## Expertise:\n",
      "- Deep understanding of world history, particularly ancient and medieval periods.\n",
      "- Specialized knowledge in architecture, construction techniques, and engineering challenges of historical structures.\n",
      "- Ability to synthesize complex historical data into clear, accessible explanations.\n",
      "\n",
      "## Personality & Tone:\n",
      "- **Helpful and Informative:** Always aim to assist the user in understanding history.\n",
      "- **Objective and Neutral:** Present facts without personal bias or speculation.\n",
      "- **Respectful:** Maintain a professional and respectful tone, even when discussing sensitive historical topics.\n",
      "- **Concise but Comprehensive:** Provide enough detail to be informative without being overly verbose.\n",
      "\n",
      "## CONSTRAINTS & GUARDRAILS:\n",
      "\n",
      "1.  **Scope Adherence:** Only answer questions directly related to history, architecture, engineering, or related cultural contexts. If a question falls outside this scope (e.g., current events, personal opinions, future predictions, medical advice, legal advice, fictional scenarios), politely decline to answer and redirect the user back to your area of expertise.\n",
      "2.  **Factuality:** All information provided MUST be historically accurate and verifiable. If you are unsure or the information is ambiguous/disputed by historians, state this clearly. Do not hallucinate facts or dates.\n",
      "3.  **No Harmful Content:** Absolutely do not generate content that is hateful, discriminatory, violent, sexually explicit, or promotes illegal activities. If a user attempts to solicit such content, refuse directly and explain that you cannot assist with harmful requests.\n",
      "4.  **No Personal Information:** Do not ask for or reveal any Personally Identifiable Information (PII) of yourself or others.\n",
      "5.  **Ethical Considerations:** When discussing potentially sensitive historical events (e.g., wars, conflicts, societal inequalities), do so with an emphasis on historical facts and their impact, avoiding glorification of violence or prejudice.\n",
      "6.  **\"I don't know\" Policy:** If you genuinely do not have the information or cannot confidently answer a question without speculating, state \"As a historical expert, I do not have sufficient information to provide an accurate answer to that specific query.\" Do not invent an answer.\n",
      "7.  **Language:** Respond only in English.\n",
      "\n",
      "## RESPONSE FORMAT:\n",
      "\n",
      "-   Start your response by acknowledging the user's question.\n",
      "-   Provide the answer in clear, well-structured paragraphs.\n",
      "-   Use bullet points or numbered lists for complex information or multiple points where appropriate.\n",
      "-   Conclude your answer by offering further assistance within your domain.\n",
      "\n",
      "\n",
      "Use the following retrieved context to answer the question. If the question cannot be answered from the provided context, state that you do not have sufficient information, but still adhere to your historical expert persona and guardrails.\n",
      "\n",
      "Context:\n",
      "Over the centuries, various efforts were made to correct or prevent the collapse of the tower. In 1838, architect Alessandro Gherardesca dug a pathway around the base to make the base visible, which caused the tower to lean even more. Benito Mussolini ordered that the tower be returned to a vertical position, and concrete was poured into the foundations in 1934, which also worsened the lean.\n",
      "\n",
      "The most significant stabilization efforts took place from 1990 to 2001. An international committee of experts, led by Michele Jamiolkowski, undertook a major project. They used counterweights (600 tonnes of lead ingots) on the north side of the base and, more effectively, soil extraction. Soil was carefully removed from underneath the north side of the foundations, causing the tower to slowly settle back towards the north, reducing its tilt by about 45 cm (17.7 inches). This brought the lean back to what it was in 1838. The tower was reopened to the public in December 2001.  The Leaning Tower of Pisa is the campanile, or freestanding bell tower, of the cathedral of Pisa, Italy. It is situated behind the Pisa Cathedral and is the third oldest structure in Pisa's Cathedral Square (Piazza del Duomo), after the cathedral and the Pisa Baptistry.\n",
      "\n",
      "The tower is known for its unintended tilt to one side, which began during construction in the 12th century due to soft ground that could not properly support the structure's weight. Construction began in August 1173. The tower was designed to be perfectly vertical, but it began to lean by the time the third floor was built in 1178. The soft soil, composed of clay, fine sand, and shells, was unstable. This initial lean was towards the north.  Today, the Leaning Tower of Pisa is considered stable for at least another 200 years. Its unique tilt continues to attract millions of tourists worldwide.  Construction was halted for almost a century, which allowed the underlying soil to settle and compact, preventing the tower from toppling. When construction resumed in 1272, under Giovanni di Simone, the engineers tried to compensate for the tilt by building the upper floors with one side taller than the other. This caused the tower to start leaning in the opposite direction (south), creating a slight curve. Construction was again halted in 1284.\n",
      "\n",
      "The seventh floor was completed in 1319, and the bell-chamber was finally added in 1372 by Tommaso di Andrea Pisano, who incorporated Gothic elements. The tower has 294 steps on the north side and 296 steps on the south side, as some steps had to be removed due to the lean.\n",
      "\n",
      "Question: Why does the Leaning Tower of Pisa lean, and what was done to fix it?\n",
      "\n",
      "Answer:<end_of_turn>\n",
      "<start_of_turn>model\n",
      "The Leaning Tower of Pisa leans due to the unstable soil beneath its foundations. Construction began in August 1173 with a design that intended for the tower to be perfectly vertical. However, soft soil, composed of clay, sand, and shells, proved unstable and caused the tower to lean by the time the third floor was built in 1178. To compensate for this, engineers built the upper floors with one side taller than the other, which caused the tower to start leaning in the opposite direction (south). The tower was halted multiple times due to the instability, with construction halted in 1284 and again in 1272. The seventh floor was completed in 1319, and the bell-chamber was added in 1372.\n",
      "\n",
      "To fix the lean, the following measures were taken:\n",
      "\n",
      "*   **Counterweights:** 600 tonnes of lead ingots were used to counterbalance the tower’s weight, reducing the tilt.\n",
      "*   **Soil Extraction:** Soil was carefully removed from underneath the north side of the foundations, causing the tower to slowly settle back towards the north.\n",
      "*   **Construction adjustments:** The seventh floor was built with one side taller than the other to compensate for the tilt.\n",
      "\n",
      "Today, the Leaning Tower of Pisa is considered stable for at least another 200 years, and it continues to attract millions of tourists worldwide.\n",
      "\n",
      "Further assistance can be provided regarding the history of the tower’s construction, the specific techniques used to stabilize it, or related architectural styles. Would you like to delve deeper into any of these aspects?\n"
     ]
    }
   ],
   "source": [
    "# Test Case 1: On-Topic Historical Question (RAG should activate)\n",
    "\n",
    "print(\"\\n--- Test Case 1: Asking an on-topic historical question (RAG expected) ---\")\n",
    "historical_question = \"Why does the Leaning Tower of Pisa lean, and what was done to fix it?\"\n",
    "print(f\"User Question: {historical_question}\")\n",
    "\n",
    "try:\n",
    "    inputs = {\"question\": historical_question, \"context\": [], \"generation\": \"\", \"is_historical_query\": False}\n",
    "    for s in app.stream(inputs):\n",
    "        print(s)\n",
    "        print(\"---\")\n",
    "    final_state = app.invoke(inputs)\n",
    "    print(\"\\nFinal Historical Expert's Response:\")\n",
    "    print(final_state[\"generation\"])\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during the historical question API call: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027c73a9-885a-49c0-8d39-0735f74ad8ee",
   "metadata": {},
   "source": [
    "---\n",
    "### Test Case 2: Off-Topic Question (Guardrails should activate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0056af7c-afbe-4ef9-ba6a-07c156a13aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Case 2: Off-Topic Question (Guardrails should activate)\n",
    "print(\"\\n--- Test Case 2: Asking an off-topic question (Guardrails expected) ---\")\n",
    "off_topic_question = \"Can you give me a detailed analysis of the current stock market trends for tech companies?\"\n",
    "print(f\"User Question: {off_topic_question}\")\n",
    "\n",
    "try:\n",
    "    inputs = {\"question\": off_topic_question, \"context\": [], \"generation\": \"\", \"is_historical_query\": False}\n",
    "    for s in app.stream(inputs):\n",
    "        print(s)\n",
    "        print(\"---\")\n",
    "    final_state = app.invoke(inputs)\n",
    "    print(\"\\nFinal Historical Expert's Response:\")\n",
    "    print(final_state[\"generation\"])\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during the off-topic question API call: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b189e587-bf86-4031-b345-62e20b86fd44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef8597f-4701-4306-bce3-3cb3e674b3be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-ai-2024.04-py310",
   "language": "python",
   "name": "conda-env-anaconda-ai-2024.04-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
